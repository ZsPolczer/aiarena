===== Project File Tree =====
├── .gitignore
├── PLAN.md
├── README.md
├── agents
│   ├── body.py
│   └── brain.py
├── arena
│   └── arena.py
├── bak
│   └── my_agent_project_20250513_123955.md
├── config.py
├── evolve
│   └── evo.py
├── main.py
├── storage
│   └── persist.py
└── ui
    └── viewer.py

===== Code and Configuration Files =====

===== config.py =====


===== main.py =====
# evo_arena/main.py
import pygame
import argparse # For CLI later
from arena.arena import Arena
from agents.body import AgentBody
from agents.brain import TinyNet # Will be needed for AI agents
from ui.viewer import Viewer
# from storage import persist # For loading/saving brains

# --- Configuration ---
ARENA_WIDTH = 800
ARENA_HEIGHT = 800
FPS = 50 

AGENT_BASE_SPEED = 200
AGENT_ROTATION_SPEED_DPS = 180
AGENT_RADIUS = 15
WALL_BOUNCE_LOSS_FACTOR = 0.9

MANUAL_AGENT_COLOR = (0, 150, 255)
DUMMY_AGENT_COLOR = (255, 100, 0)
AI_AGENT_COLOR = (0, 200, 50) # Green for AI

# --- New Weapon Defaults ---
WEAPON_RANGE = 150            # Increased from 80
WEAPON_ARC_DEG = 90           # Reduced from 120 (narrower cone)
WEAPON_COOLDOWN_TIME = 0.6
WEAPON_DAMAGE = 25            # Increased from 10

def run_manual_simulation():
    """Runs the simulation with one manual agent and one dummy."""
    game_arena = Arena(ARENA_WIDTH, ARENA_HEIGHT, wall_bounce_loss_factor=WALL_BOUNCE_LOSS_FACTOR)

    manual_agent = AgentBody(
        x=100, y=ARENA_HEIGHT / 2,
        angle_deg=0,
        base_speed=AGENT_BASE_SPEED,
        rotation_speed_dps=AGENT_ROTATION_SPEED_DPS,
        radius=AGENT_RADIUS,
        color=MANUAL_AGENT_COLOR,
        agent_id="player",
        team_id=1,
        hp=100,
        brain=None, # Manual control
        weapon_range=WEAPON_RANGE,             # Using new constant
        weapon_arc_deg=WEAPON_ARC_DEG,         # Using new constant
        weapon_cooldown_time=WEAPON_COOLDOWN_TIME,
        weapon_damage=WEAPON_DAMAGE            # Using new constant
    )
    game_arena.add_agent(manual_agent)

    dummy_target = AgentBody(
        x=ARENA_WIDTH - 100, y=ARENA_HEIGHT / 2,
        angle_deg=180,
        base_speed=0,
        rotation_speed_dps=0,
        radius=AGENT_RADIUS + 5,
        color=DUMMY_AGENT_COLOR,
        agent_id="dummy",
        team_id=2,
        hp=200, # Dummy has more HP
        is_dummy=True,
        brain=None,
        weapon_range=0, # Dummy has no weapon
        weapon_arc_deg=0,
        weapon_cooldown_time=999,
        weapon_damage=0
    )
    game_arena.add_agent(dummy_target)
    
    # Add a stationary AI agent for testing targeting (optional)
    # stationary_ai_brain = TinyNet() # Default random brain
    # stationary_ai_agent = AgentBody(
    #     x=ARENA_WIDTH / 2, y=ARENA_HEIGHT - 100,
    #     angle_deg=-90, base_speed=AGENT_BASE_SPEED, rotation_speed_dps=AGENT_ROTATION_SPEED_DPS,
    #     radius=AGENT_RADIUS, color=AI_AGENT_COLOR, agent_id="ai_stationary", team_id=2, hp=100,
    #     brain=stationary_ai_brain, # Give it a brain
    #     weapon_range=WEAPON_RANGE, weapon_arc_deg=WEAPON_ARC_DEG,
    #     weapon_cooldown_time=WEAPON_COOLDOWN_TIME, weapon_damage=WEAPON_DAMAGE
    # )
    # game_arena.add_agent(stationary_ai_agent)


    game_viewer = Viewer(ARENA_WIDTH, ARENA_HEIGHT, game_arena)
    game_viewer.run_simulation_loop(FPS, manual_agent_id="player") # Pass arena via constructor

# --- Main execution flow ---
def main():
    # For now, directly call the manual simulation.
    # Later, we'll use argparse to select modes like 'train', 'show', 'match'.
    parser = argparse.ArgumentParser(description="Evo Arena: A simple agent evolution project.")
    parser.add_argument('mode', nargs='?', default='manual', choices=['manual', 'train', 'show', 'match'],
                        help="Mode to run: 'manual' (default), 'train', 'show' a replay, or 'match' agents.")
    # Add more arguments for specific modes later

    args = parser.parse_args()

    if args.mode == 'manual':
        run_manual_simulation()
    # elif args.mode == 'train':
    #     run_training_session() # To be implemented
    else:
        print(f"Mode '{args.mode}' not fully implemented yet. Running manual simulation.")
        run_manual_simulation()


if __name__ == '__main__':
    main()

===== PLAN.md =====
Okay, this is a fantastic and well-structured plan! It's ambitious but definitely achievable for a focused 1-2 day hobby project. Let's break this down into a concrete development plan, keeping it lean.

I'll follow your "Day-by-day roadmap" as the main structure and flesh out the steps within each.

---

## Development Plan

**Core Principle:** Implement the simplest thing that works for each step. Get a feature working end-to-end before over-engineering or polishing. We'll stick to NumPy and PyGame as requested.

**Global Setup (Before Day 0):**

1.  **Project Setup:**
    *   Create a main project directory (e.g., `evo_arena`).
    *   Initialize a Git repository: `git init`.
    *   Create the basic directory structure: `arena/`, `agents/`, `evolve/`, `ui/`, `storage/`.
    *   Create empty `__init__.py` files in each subdirectory to make them Python packages.
    *   Create `main.py` at the root.
    *   Create placeholder files: `arena/arena.py`, `agents/brain.py`, `agents/body.py`, `evolve/evo.py`, `ui/viewer.py`, `storage/persist.py`.
2.  **Environment:**
    *   Ensure Python ≥ 3.10 is installed.
    *   Install dependencies: `pip install numpy pygame`.
3.  **Constants:**
    *   Create a `config.py` (or similar, or just put them at the top of `arena.py` for now) to hold game constants (arena size, agent speed, weapon specs, etc.) for easy tweaking.

---

### Phase 1: Manual Control & Basic Arena (Day 0 - Evening Goal)

**Goal:** Get a PyGame window showing an arena, one manually controlled agent, and a dummy target.

1.  **`arena/arena.py` - Basic Arena Logic:**
    *   `Arena` class:
        *   `__init__(self, width=800, height=800)`: Store dimensions.
        *   `add_agent(self, agent)`: Method to add agents (initially just one manual, one dummy).
        *   `update(self, dt)`:
            *   Loop through agents and call their `update` method (to be created in `body.py`).
            *   Implement wall collision:
                *   Check if agent's new position is outside bounds.
                *   If so, "bounce back" (invert appropriate velocity component) and reduce speed by 10%.
                *   Clamp position to be within bounds after bounce to prevent sticking.
    *   *Initial `dt` will be based on PyGame's clock tick.*

2.  **`agents/body.py` - Manual Agent Body:**
    *   `AgentBody` class:
        *   `__init__(self, x, y, angle, speed, team_id=0)`: Store position, orientation (angle in radians/degrees), speed, HP (e.g., 100).
        *   `vx`, `vy`: Current velocity components.
        *   `manual_control(self, keys)`:
            *   Update `vx`, `vy` based on arrow keys (e.g., up/down for forward/backward thrust, left/right for rotation).
            *   Rotation changes `angle`.
            *   Forward/backward thrust sets `vx = cos(angle) * speed`, `vy = sin(angle) * speed` (or modifies existing velocity if you want acceleration, but spec says "teleport-step"). For now, let's make actions set desired velocity for next step.
        *   `update(self, dt)`:
            *   `self.x += self.vx * dt`
            *   `self.y += self.vy * dt`
            *   *Initially, weapon logic is not needed.*

3.  **`ui/viewer.py` - PyGame Visualization:**
    *   `Viewer` class:
        *   `__init__(self, arena)`: Store arena reference. Initialize PyGame window.
        *   `draw_arena(self)`: Draw the arena boundary.
        *   `draw_agent(self, agent)`: Draw an agent as a triangle or circle with a line indicating direction.
        *   `run_manual_loop(self, manual_agent, dummy_agent)`:
            *   Main PyGame loop:
                *   Handle events (quit, keyboard input).
                *   Pass keyboard state to `manual_agent.manual_control()`.
                *   Call `arena.update(dt)` (where `dt` is from PyGame clock, e.g., `1/50.0`).
                *   Clear screen.
                *   Call `draw_arena()`.
                *   Draw `manual_agent` and `dummy_agent`.
                *   `pygame.display.flip()`.
                *   `pygame.time.Clock().tick(50)` (for 50 Hz).

4.  **`main.py` - Entry Point for Manual Mode:**
    *   Import necessary classes.
    *   Create `Arena` instance.
    *   Create one `AgentBody` for manual control, another `AgentBody` as a static dummy target.
    *   Add agents to the arena.
    *   Create `Viewer` instance.
    *   Call `viewer.run_manual_loop()`.

---

### Phase 2: Neural Net Agents & Headless Evolution (Day 1)

**Goal:** Implement `TinyNet`, integrate it with `AgentBody`, create the evolution loop, and run it headlessly for 20 generations. View fitness.

1.  **`agents/brain.py` - Neural Network:**
    *   Copy-paste the `TinyNet` class from the spec.
    *   Initialize `rng = np.random.default_rng(seed=None)` at module level or pass it around for determinism. (For now, `np.random` is fine for speed).

2.  **`agents/body.py` - AI Agent Body Enhancements:**
    *   Modify `AgentBody`:
        *   `__init__(...)`: Add `brain` parameter (a `TinyNet` instance). Add `weapon_cooldown_timer = 0`, `max_cooldown = 0.6`.
        *   `get_inputs(self, arena)`:
            *   This is crucial. It needs to find the nearest enemy and ally (if any).
            *   For 1v1, "ally" isn't relevant yet, but design for it.
            *   Calculate distances and bearings (sin/cos θ). Normalize to \[-1, 1].
            *   Implement "±1 if none in sight" logic.
            *   Own health / 100.
            *   Weapon ready? (1 if `weapon_cooldown_timer <= 0`, -1 otherwise).
            *   x-velocity, y-velocity (normalized/clamped).
            *   Bias neuron = 1.
            *   Return the 14-element NumPy array.
        *   `act(self, outputs)`:
            *   Take the 4 outputs from `TinyNet`.
            *   Threshold them (≥ 0 vs < 0).
            *   Translate to actions:
                *   Thrust: Adjust `vx`, `vy` based on current `angle` and `speed`.
                *   Strafe: Adjust `vx`, `vy` based on `angle + pi/2` or `angle - pi/2`.
                *   Rotate: Adjust `angle`.
                *   Fire: If `weapon_cooldown_timer <= 0`, set a flag `is_firing = True` and reset `weapon_cooldown_timer = self.max_cooldown`.
        *   Modify `update(self, dt)`:
            *   If `brain` exists:
                *   `inputs = self.get_inputs(arena)`
                *   `outputs = self.brain(inputs)`
                *   `self.act(outputs)`
            *   Decrement `weapon_cooldown_timer` if > 0.
            *   `self.x += self.vx * dt`
            *   `self.y += self.vy * dt`
            *   Reset `is_firing = False` after physics step.

3.  **`arena/arena.py` - Game Logic Enhancements:**
    *   Modify `Arena`:
        *   `update(self, dt)`:
            *   After agents update their positions:
                *   Handle weapon firing:
                    *   For each agent that `is_firing`:
                        *   Check for hits on other agents (enemies).
                        *   A hit occurs if an enemy is within the 120° cone and 80m range.
                        *   (Simple collision detection: distance check + angle check).
                        *   If hit, reduce target's HP by 10.
            *   Check match end conditions:
                *   One agent (or team) left.
                *   60s timeout (`self.game_time += dt`).
                *   Return a status (e.g., winner_id, 'draw').
        *   `reset(self)`: Method to clear agents, reset game time, etc., for new matches.
        *   Need a method to run a single match: `run_match(agent1_brain, agent2_brain, max_duration=60)`:
            *   Reset arena.
            *   Create two `AgentBody` instances, assign them the provided brains.
            *   Add them to the arena.
            *   Loop `max_duration / dt` times (e.g., 60s * 50Hz = 3000 steps):
                *   `arena.update(dt)`.
                *   If match ends, break and return winner/scores.
            *   Return final state (HP of both agents, who won).

4.  **`evolve/evo.py` - Evolution Loop:**
    *   `EvolutionOrchestrator` class:
        *   `__init__(self, population_size=64, num_elites=8, mutation_rate=0.9, mutation_sigma=0.2, target_fitness_stdev=0.03)`: Store params.
        *   `population`: A list of `TinyNet` instances (genomes).
        *   `initialize_population(self)`: Create `population_size` random `TinyNet`s.
        *   `run_tournament(self, genome1, genome2, arena_instance)`:
            *   Use `arena_instance.run_match(genome1.brain, genome2.brain)` (or directly pass genome if `TinyNet` is the genome).
            *   Return fitness components for genome1.
        *   `evaluate_population(self, arena_instance)`:
            *   For each genome in `population`:
                *   Initialize its fitness to 0.
                *   Play 4 matches (1v1) against random opponents from the current population.
                *   `fitness += num_wins + 0.1 * (HP_left - HP_enemy)`.
                *   Store fitness with the genome (e.g., `genome.fitness = ...`).
        *   `select_and_reproduce(self)`:
            *   Sort population by fitness.
            *   Copy top `num_elites` to the new population.
            *   For the remaining slots:
                *   With `mutation_rate` chance:
                    *   Select one parent (e.g., from top 50%).
                    *   `new_genome = parent.mutate(sigma=self.mutation_sigma)` (add `mutate` method to `TinyNet`).
                *   Else (crossover):
                    *   Select two parents.
                    *   `new_genome = TinyNet.crossover(parent1, parent2)` (add `crossover` method to `TinyNet`).
            *   Replace old population with new.
        *   `diversity_guard(self)`:
            *   Calculate stdev of fitness scores.
            *   If stdev < `target_fitness_stdev`, increase `self.mutation_sigma` (e.g., `*= 1.1`).
        *   `evolve(self, generations=20, arena_instance)`:
            *   Main loop:
                *   `self.evaluate_population(arena_instance)`.
                *   Log best/avg fitness (print to console, or basic CSV).
                *   `self.select_and_reproduce()`.
                *   `self.diversity_guard()`.
                *   Every 10 generations: call `persist.save_genome()` for the best genome.

5.  **`agents/brain.py` - `TinyNet` modifications:**
    *   `mutate(self, sigma)`:
        *   Create new `w_in_mutated = self.w_in + np.random.normal(0, sigma, self.w_in.shape)`.
        *   Create new `w_out_mutated = self.w_out + np.random.normal(0, sigma, self.w_out.shape)`.
        *   Return `TinyNet(w_in_mutated, w_out_mutated)`.
    *   `crossover(cls, parent1, parent2)` (class method):
        *   `w_in_child = np.where(np.random.rand(*parent1.w_in.shape) < 0.5, parent1.w_in, parent2.w_in)` (uniform crossover).
        *   `w_out_child = np.where(np.random.rand(*parent1.w_out.shape) < 0.5, parent1.w_out, parent2.w_out)`.
        *   Return `TinyNet(w_in_child, w_out_child)`.

6.  **`storage/persist.py` - Genome Storage:**
    *   `save_genome(genome, filename)`:
        *   `np.savez(filename, w_in=genome.w_in, w_out=genome.w_out, fitness=getattr(genome, 'fitness', -1))`.
    *   `load_genome(filename)`:
        *   `data = np.load(filename)`.
        *   Return `TinyNet(data['w_in'], data['w_out'])`. (Fitness can be retrieved if needed).

7.  **`main.py` - `train` Command:**
    *   Add argument parsing (e.g., `argparse`) for a `train` command.
    *   `if args.command == 'train'`:
        *   Create `Arena` instance.
        *   Create `EvolutionOrchestrator` instance.
        *   `orchestrator.initialize_population()`.
        *   `orchestrator.evolve(generations=args.iters, arena_instance=arena)`.
    *   To view fitness: For now, just print to console. A separate simple script can plot the CSV later if you log to CSV.

8.  **(Stretch) `storage/persist.py` - JSONL Replay:**
    *   In `Arena.run_match()`:
        *   At each step (or every N steps), collect state: `{'t': self.game_time, 'agents': [{'id': a.id, 'x': a.x, 'y': a.y, 'angle': a.angle, 'hp': a.hp} for a in self.agents]}`.
        *   Append this as a JSON line to a replay file.
    *   **(Stretch) `ui/viewer.py` or `main.py show_replay` - Simple Playback:**
        *   Read the JSONL file.
        *   For each entry, clear PyGame screen and draw agents based on the logged state.
        *   Control playback speed.

---

### Phase 3: Interactive Matches & Advanced Modes (Day 2)

**Goal:** Visualize 1v1 matches between specific agents, add FFA and Team modes.

1.  **`main.py` - `match` Command:**
    *   Add `match <genome1.npz> <genome2.npz>` command using `argparse`.
    *   Load the two genomes using `persist.load_genome()`.
    *   Create `Arena` instance.
    *   Create `Viewer` instance.
    *   `run_visual_match(self, arena, viewer, brain1, brain2)`:
        *   Similar to `Arena.run_match()` but integrates with `Viewer` for live display.
        *   In the loop:
            *   PyGame event handling.
            *   Agents get inputs, brains decide actions, bodies `act`.
            *   `arena.update(dt)`.
            *   `viewer.draw_everything()`.
            *   `pygame.display.flip()`.
            *   `pygame.time.Clock().tick(50)`.
            *   Check for match end.
    *   Use this to pit "gen 0 vs gen 200" (by saving gen 0 best and gen 200 best).

2.  **Game Mode Flags (`--ffa`, `--teams N`) in `main.py`:**
    *   Add these flags to `argparse` for the `train` command (and potentially for a new `show` command that can run different modes).

3.  **`arena/arena.py` & `evolve/evo.py` - FFA Mode:**
    *   **Arena:**
        *   `run_match()` needs to handle N agents, not just 2.
        *   Match ends when 1 agent is left or timeout.
        *   No concept of "enemy" vs "ally" in `AgentBody.get_inputs()`'s target selection for FFA; all others are "enemies".
    *   **Evolve:**
        *   `evaluate_population()` for FFA:
            *   Instead of 1v1, could do small FFA matches (e.g., 4 agents per match).
            *   Fitness is individual (e.g., survival time, damage dealt, KOs). Simpler: fitness = 1 if win, 0 if loss, + scaled HP_left.

4.  **`arena/arena.py` & `evolve/evo.py` - Team Mode:**
    *   **AgentBody:**
        *   `__init__`: Add `team_id`.
        *   `get_inputs()`: Must correctly identify nearest enemy *and* nearest ally based on `team_id`.
    *   **Arena:**
        *   `run_match()` needs to know team assignments.
        *   Match ends when one team is eliminated or timeout.
    *   **Evolve:**
        *   `evaluate_population()` for Team mode:
            *   Setup team matches (e.g., two teams of 3).
            *   **Shared team reward:** All members of the winning team get the same fitness outcome (e.g., win/loss points, team total HP difference).
            *   Example: `run_team_match(team1_genomes, team2_genomes, arena_instance)`.

5.  **(Stretch) Polish CLI, ZIP Checkpoints, README:**
    *   **CLI:** Refine `argparse` in `main.py` for all options (iterations, headless, game modes, file paths).
    *   **ZIP Checkpoints:** Modify `persist.save_genome` to potentially save to a zip if multiple files per checkpoint (e.g., genome + metadata JSON). For now, `.npz` is fine.
    *   **README.md:** Basic instructions on how to run `train`, `match`.

---

### Final Checklist Considerations (Throughout Development):

*   **Determinism:**
    *   Use `rng = np.random.default_rng(seed)`: Instantiate one `rng` object and pass it around or make it globally accessible (e.g., in `config.py`). Use `rng.uniform()`, `rng.normal()`, `rng.choice()` etc. This is key for reproducible replays and evolution.
    *   Fixed time step (50 Hz / 20 ms) is already planned.
*   **Configuration:**
    *   CLI args are a good start. For a 1-2 day project, a YAML file might be overkill but consider it if you find yourself with too many CLI options. Key parameters like mutation rates, population size, arena size should be easily tweakable.
*   **Clarity/Docs:**
    *   Good variable names.
    *   Comments for complex logic, especially in `arena.py` (hit detection, game rules) and `evolve.py` (selection, mutation).
    *   The current spec is a great "external" doc; internal code should be readable.

---

This plan breaks down the project into manageable chunks. Focus on getting one part working before moving to the next. For example, for Day 1, get *one* agent controlled by a *random* `TinyNet` moving in the arena *before* implementing the full evolution loop. Then add a second agent for 1v1, then the evolution.

Good luck, this looks like a super fun project!

===== README.md =====
Failed to read README.md: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

===== agents/body.py =====
# evo_arena/agents/body.py
import pygame
import math
import numpy as np # For brain inputs

# Assuming TinyNet might be passed in, or loaded
# from agents.brain import TinyNet # Not strictly needed here if brain is passed as object

class AgentBody:
    def __init__(self, x, y, angle_deg, base_speed, rotation_speed_dps,
                 radius=15, color=(0, 0, 255), agent_id="agent", team_id=0,
                 hp=100, brain=None, is_dummy=False,
                 weapon_range=80, weapon_arc_deg=120, weapon_cooldown_time=0.6, weapon_damage=10):
        
        self.agent_id = str(agent_id)
        self.team_id = int(team_id)
        self.x = float(x)
        self.y = float(y)
        self.angle_deg = float(angle_deg)
        self.radius = int(radius)
        self.color = color
        self.is_dummy = is_dummy
        self.brain = brain # This will be a TinyNet instance or None

        self.base_speed = float(base_speed)
        self.rotation_speed_dps = float(rotation_speed_dps) # Degrees per second for turning
        
        self.vx = 0.0
        self.vy = 0.0
        self.current_speed = 0.0 # Current forward/backward speed component

        self.max_hp = float(hp)
        self.hp = float(hp)
        
        # Weapon attributes
        self.weapon_range = float(weapon_range)
        self.weapon_arc_deg = float(weapon_arc_deg) # Centered on agent's forward direction
        self.weapon_cooldown_time = float(weapon_cooldown_time) # Seconds
        self.weapon_damage = int(weapon_damage)
        self.weapon_cooldown_timer = 0.0 # Counts down to 0 when ready
        self.is_firing_command = False # Set by brain/manual control, processed in arena

        # For manual control state (if brain is None)
        self._manual_thrust_forward = False
        self._manual_thrust_backward = False
        self._manual_rotate_left = False
        self._manual_rotate_right = False
        self._manual_fire = False
        
        # Clamping values for NN inputs/outputs if needed
        self.max_abs_velocity_component = self.base_speed # For normalizing velocity inputs

    def is_alive(self):
        return self.hp > 0

    def take_damage(self, amount):
        self.hp -= amount
        if self.hp < 0:
            self.hp = 0
            print(f"Agent {self.agent_id} defeated.")

    def manual_control(self, keys):
        if self.is_dummy or self.brain is not None: # Manual control only if no brain
            return

        self._manual_thrust_forward = keys[pygame.K_UP]
        self._manual_thrust_backward = keys[pygame.K_DOWN]
        self._manual_rotate_left = keys[pygame.K_LEFT]
        self._manual_rotate_right = keys[pygame.K_RIGHT]
        self._manual_fire = keys[pygame.K_SPACE] # Example: Space to fire

    def get_inputs(self, arena_width, arena_height, all_agents):
        """
        Generates the 14 input values for the neural network, normalized to [-1, 1].
        """
        inputs = np.zeros(14) # 14 inputs as per spec

        # Helper for normalization
        def normalize(value, min_val, max_val):
            if max_val == min_val: return 0.0 # Avoid division by zero
            return np.clip(2 * (value - min_val) / (max_val - min_val) - 1, -1.0, 1.0)

        # Helper to find nearest agent (enemy or ally)
        def find_nearest_agent_in_list(target_list):
            nearest_dist = float('inf')
            nearest_agent_obj = None
            for other_agent in target_list:
                if other_agent is self or not other_agent.is_alive():
                    continue
                dx = other_agent.x - self.x
                dy = other_agent.y - self.y
                dist = math.sqrt(dx**2 + dy**2) - self.radius - other_agent.radius # Edge to edge
                if dist < nearest_dist:
                    nearest_dist = dist
                    nearest_agent_obj = other_agent
            return nearest_agent_obj, nearest_dist

        # 1. Forward distance to nearest enemy
        # 2. Bearing to that enemy (sin θ, cos θ)
        enemies = [a for a in all_agents if a.team_id != self.team_id and a.is_alive()]
        nearest_enemy, enemy_dist = find_nearest_agent_in_list(enemies)
        
        max_view_dist = math.sqrt(arena_width**2 + arena_height**2) # Diagonal of arena

        if nearest_enemy:
            inputs[0] = normalize(enemy_dist, 0, max_view_dist) # Normalize distance
            
            dx_enemy = nearest_enemy.x - self.x
            dy_enemy = nearest_enemy.y - self.y
            angle_to_enemy_rad = math.atan2(dy_enemy, dx_enemy)
            
            # Bearing relative to agent's current orientation
            # Angle of agent: self.angle_deg (convert to rad)
            # Relative angle = angle_to_enemy - agent_angle
            relative_angle_rad = angle_to_enemy_rad - math.radians(self.angle_deg)
            # Normalize angle to [-pi, pi]
            relative_angle_rad = (relative_angle_rad + math.pi) % (2 * math.pi) - math.pi
            
            inputs[1] = math.sin(relative_angle_rad) # Already in [-1, 1]
            inputs[2] = math.cos(relative_angle_rad) # Already in [-1, 1]
        else:
            inputs[0] = 1.0 # No enemy in sight (or -1.0, spec says +/-1)
            inputs[1] = 0.0 # No bearing if no enemy
            inputs[2] = 0.0 

        # 3. Forward distance to nearest ally
        # 4. Bearing to that ally (sin θ, cos θ)
        allies = [a for a in all_agents if a.team_id == self.team_id and a is not self and a.is_alive()]
        nearest_ally, ally_dist = find_nearest_agent_in_list(allies)

        if nearest_ally:
            inputs[3] = normalize(ally_dist, 0, max_view_dist)
            
            dx_ally = nearest_ally.x - self.x
            dy_ally = nearest_ally.y - self.y
            angle_to_ally_rad = math.atan2(dy_ally, dx_ally)
            relative_angle_ally_rad = angle_to_ally_rad - math.radians(self.angle_deg)
            relative_angle_ally_rad = (relative_angle_ally_rad + math.pi) % (2 * math.pi) - math.pi
            
            inputs[4] = math.sin(relative_angle_ally_rad)
            inputs[5] = math.cos(relative_angle_ally_rad)
        else:
            inputs[3] = 1.0 # No ally in sight
            inputs[4] = 0.0
            inputs[5] = 0.0

        # 5. Own health / 100 (normalized to [0,1], spec says [-1,1], so scale if max_hp is 100)
        inputs[6] = normalize(self.hp, 0, self.max_hp) # Will be [-1,1] if hp can be 0 to max_hp

        # 6. Weapon ready? (1 = yes, –1 = cooling)
        inputs[7] = 1.0 if self.weapon_cooldown_timer <= 0 else -1.0

        # 7. x-velocity, y-velocity (clamped and normalized)
        # Max possible speed is base_speed for now.
        # These are world-frame velocities.
        inputs[8] = normalize(self.vx, -self.max_abs_velocity_component, self.max_abs_velocity_component)
        inputs[9] = normalize(self.vy, -self.max_abs_velocity_component, self.max_abs_velocity_component)
        
        # What are inputs 10, 11, 12? Spec shows 14 total.
        # My list:
        # 0: enemy_dist
        # 1: enemy_bearing_sin
        # 2: enemy_bearing_cos
        # 3: ally_dist
        # 4: ally_bearing_sin
        # 5: ally_bearing_cos
        # 6: health
        # 7: weapon_ready
        # 8: vx
        # 9: vy
        # --> This is 10 inputs. The spec image shows 14 inputs total.
        # Let's check the image again: "Forward distance", "Bearing (sin, cos)", "Forward distance (ally)", "Bearing (ally) (sin,cos)" = 6
        # "Own health" = 1
        # "Weapon ready" = 1
        # "x-velocity, y-velocity" = 2
        # "Bias neuron = 1" = 1
        # Total = 6+1+1+2+1 = 11.
        # The TinyNet code has `(16, 14)` for `w_in`. So 14 inputs.
        # "Inputs (all normalised to [-1, 1]) – 14 numbers total:"
        # 1. Forward distance to nearest enemy (±1 if none in sight) -> 1 number
        # 2. Bearing to that enemy (sin θ, cos θ) -> 2 numbers
        # 3. Forward distance to nearest ally (or ±1) -> 1 number
        # 4. Bearing to that ally (sin θ, cos θ) -> 2 numbers
        # 5. Own health / 100 -> 1 number
        # 6. Weapon ready? (1 = yes, –1 = cooling) -> 1 number
        # 7. x-velocity, y-velocity (clamped) -> 2 numbers
        # 8. Bias neuron = 1 -> 1 number
        # TOTAL: 1+2+1+2+1+1+2+1 = 11 inputs.
        #
        # The TinyNet diagram shows `14 x 16`.
        # The provided `TinyNet` python code `np.random.uniform(-1, 1, (16, 14))` implies 14 inputs.
        # Let's assume the list is the source of truth for the *meaning* of inputs,
        # and if there are more inputs needed by TinyNet, they are just unused or need clarification.
        # For now, I will make the input vector 14 long and pad the rest with 0, except the bias.
        #
        # Re-evaluating the spec text description for inputs:
        # 1. Fwd dist enemy (1)
        # 2. Bearing enemy (sin, cos) (2)
        # 3. Fwd dist ally (1)
        # 4. Bearing ally (sin, cos) (2)
        # 5. Own health (1)
        # 6. Weapon ready (1)
        # 7. x-velocity, y-velocity (2)
        # 8. Bias neuron (1)
        # This sums to 11 inputs.
        # The TinyNet init `self.w_in = w_in if w_in is not None else np.random.uniform(-1, 1, (16, 14))`
        # clearly expects 14 inputs.
        # The example calculation `14 × 16 + 16 × 4 = 320 parameters` also uses 14.
        #
        # Possible missing inputs / interpretation:
        # - Own angle (sin, cos)? (2)
        # - Distance to nearest wall in N directions?
        # - Time since last fired?
        #
        # For now, I'll stick to the 11 defined inputs and pad the input vector to 14.
        # The last one must be the bias neuron.
        
        # Inputs indices used so far: 0-9
        # We need 14 inputs. Last one is bias.
        # inputs[10], inputs[11], inputs[12] are currently unused. Let's set them to 0.
        inputs[10] = 0.0 # Unused / Placeholder
        inputs[11] = 0.0 # Unused / Placeholder
        inputs[12] = 0.0 # Unused / Placeholder

        # Last input: Bias neuron = 1
        inputs[13] = 1.0
        
        return inputs


    def perform_actions_from_outputs(self, outputs, dt):
        """
        Applies actions based on the 4 NN outputs.
        Outputs are real values, thresholded at 0.
        """
        if not self.is_alive():
            self.vx, self.vy = 0,0
            self.current_speed = 0
            return

        # Outputs: o1, o2, o3, o4
        # o1: thrust forward / back
        # o2: strafe left / right
        # o3: rotate left / right
        # o4: fire / hold fire

        # --- Rotation (o3) ---
        rotation_input = outputs[2]
        if rotation_input >= 0: # Rotate left
            self.angle_deg -= self.rotation_speed_dps * dt
        else: # Rotate right
            self.angle_deg += self.rotation_speed_dps * dt
        self.angle_deg %= 360

        # --- Movement (o1: thrust, o2: strafe) ---
        thrust_input = outputs[0]
        strafe_input = outputs[1]
        
        target_vx, target_vy = 0.0, 0.0
        agent_angle_rad = math.radians(self.angle_deg)

        # Thrust
        if thrust_input >= 0: # Thrust forward
            target_vx += self.base_speed * math.cos(agent_angle_rad)
            target_vy += self.base_speed * math.sin(agent_angle_rad)
        else: # Thrust backward
            target_vx -= (self.base_speed / 2) * math.cos(agent_angle_rad) # Slower backward
            target_vy -= (self.base_speed / 2) * math.sin(agent_angle_rad)

        # Strafe
        strafe_speed_factor = 0.75 # Strafe a bit slower than forward thrust
        if strafe_input >= 0: # Strafe left
            # Left is agent_angle_rad - PI/2
            strafe_angle_rad = agent_angle_rad - math.pi / 2
            target_vx += self.base_speed * strafe_speed_factor * math.cos(strafe_angle_rad)
            target_vy += self.base_speed * strafe_speed_factor * math.sin(strafe_angle_rad)
        else: # Strafe right
            # Right is agent_angle_rad + PI/2
            strafe_angle_rad = agent_angle_rad + math.pi / 2
            target_vx += self.base_speed * strafe_speed_factor * math.cos(strafe_angle_rad)
            target_vy += self.base_speed * strafe_speed_factor * math.sin(strafe_angle_rad)
        
        # For "teleport-step", we directly set velocity for this frame
        self.vx = target_vx
        self.vy = target_vy
        
        # Update current_speed for consistency if other parts rely on it (e.g. bounce reduction)
        # This is a bit tricky as vx, vy are now composed. We can store the magnitude.
        self.current_speed = math.sqrt(self.vx**2 + self.vy**2)
        if (thrust_input < 0 or (strafe_input != 0 and thrust_input == 0)): # If primarily moving backward or only strafing
             # Heuristic: if dominant movement is backward or purely sideways, consider speed negative for some logic
             # This is not well-defined by vx,vy. Let's use thrust_input to determine "forward" intent for current_speed sign.
             if thrust_input < 0 and self.current_speed > 0: # Moving backward
                 self.current_speed *= -0.5 # Match backward thrust factor
             elif thrust_input == 0 and strafe_input != 0:
                 pass # Pure strafe, speed is positive magnitude
        elif thrust_input < 0: # Moving backward
            self.current_speed *= -1 # This might be an oversimplification

        # --- Firing (o4) ---
        fire_input = outputs[3]
        if fire_input >= 0: # Fire
            if self.weapon_cooldown_timer <= 0:
                self.is_firing_command = True
                self.weapon_cooldown_timer = self.weapon_cooldown_time
            else:
                self.is_firing_command = False # Can't fire, on cooldown
        else: # Hold fire
            self.is_firing_command = False


    def update(self, dt, arena_width=None, arena_height=None, all_agents=None):
        if not self.is_alive():
            return

        # Decrement weapon cooldown
        if self.weapon_cooldown_timer > 0:
            self.weapon_cooldown_timer -= dt
            if self.weapon_cooldown_timer < 0:
                self.weapon_cooldown_timer = 0
        
        # --- Decision Making ---
        if self.brain:
            if arena_width is None or arena_height is None or all_agents is None:
                # This check is important because get_inputs needs these.
                # For simple manual mode, these might not be passed.
                # print("Warning: Agent brain active but not all arena info provided to update.")
                pass # Or raise error, or skip brain update for this frame
            else:
                inputs = self.get_inputs(arena_width, arena_height, all_agents)
                outputs = self.brain(inputs)
                self.perform_actions_from_outputs(outputs, dt)
        elif not self.is_dummy: # Manual control
            # Rotation
            if self._manual_rotate_left: self.angle_deg -= self.rotation_speed_dps * dt
            if self._manual_rotate_right: self.angle_deg += self.rotation_speed_dps * dt
            self.angle_deg %= 360
            
            # Velocity based on thrust
            current_manual_speed = 0.0
            if self._manual_thrust_forward: current_manual_speed = self.base_speed
            elif self._manual_thrust_backward: current_manual_speed = -self.base_speed / 2
            
            angle_rad = math.radians(self.angle_deg)
            self.vx = current_manual_speed * math.cos(angle_rad)
            self.vy = current_manual_speed * math.sin(angle_rad)
            self.current_speed = current_manual_speed # Store the signed speed

            # Firing for manual
            if self._manual_fire and self.weapon_cooldown_timer <= 0:
                self.is_firing_command = True
                self.weapon_cooldown_timer = self.weapon_cooldown_time
            else:
                self.is_firing_command = False # Reset if not firing or on cooldown
        else: # Dummy
            self.vx, self.vy = 0,0
            self.current_speed = 0
            self.is_firing_command = False

        # --- Position Update (Teleport-step) ---
        self.x += self.vx * dt
        self.y += self.vy * dt
        
        # Reset manual states (they are read fresh from keys each frame by viewer)
        # self._manual_thrust_forward = False ... (not needed here)

    def draw(self, screen):
        if not self.is_alive():
            # Optionally draw a wreck or nothing
            pygame.draw.circle(screen, (50,50,50), (int(self.x), int(self.y)), self.radius, 2) # Grey outline
            return

        # Draw body (circle)
        pygame.draw.circle(screen, self.color, (int(self.x), int(self.y)), self.radius)
        
        # Draw direction line
        angle_rad = math.radians(self.angle_deg)
        end_x = self.x + self.radius * math.cos(angle_rad)
        end_y = self.y + self.radius * math.sin(angle_rad)
        pygame.draw.line(screen, (255, 255, 255), (int(self.x), int(self.y)), (int(end_x), int(end_y)), 2)

        # Draw HP bar (optional)
        if self.max_hp > 0:
            hp_bar_width = self.radius * 2
            hp_bar_height = 5
            hp_bar_x = self.x - self.radius
            hp_bar_y = self.y - self.radius - hp_bar_height - 2 # Above agent

            current_hp_width = (self.hp / self.max_hp) * hp_bar_width
            pygame.draw.rect(screen, (255,0,0), (hp_bar_x, hp_bar_y, hp_bar_width, hp_bar_height)) # Red background
            pygame.draw.rect(screen, (0,255,0), (hp_bar_x, hp_bar_y, current_hp_width, hp_bar_height)) # Green foreground
        
        # Draw weapon cooldown indicator (optional)
        if self.weapon_cooldown_timer > 0:
            cooldown_arc_angle = (self.weapon_cooldown_timer / self.weapon_cooldown_time) * 360
            if cooldown_arc_angle > 0:
                try:
                    # Simplified: draw a small arc or circle segment
                    rect = pygame.Rect(self.x - self.radius/2, self.y - self.radius/2, self.radius, self.radius)
                    pygame.draw.arc(screen, (200,200,0), rect, 0, math.radians(cooldown_arc_angle), 2) # Yellow
                except TypeError as e: # pygame.draw.arc can be picky with small angles
                    # print(f"Warning: Pygame arc drawing issue: {e}")
                    pass


    def get_state_for_replay(self):
        """Returns a serializable dict of the agent's current state for replay."""
        return {
            'id': self.agent_id,
            'team_id': self.team_id,
            'x': round(self.x, 2),
            'y': round(self.y, 2),
            'angle_deg': round(self.angle_deg, 2),
            'hp': round(self.hp, 1),
            'is_firing': self.is_firing_command, # Log if trying to fire this frame
            'vx': round(self.vx, 2),
            'vy': round(self.vy, 2)
        }

===== agents/brain.py =====
# evo_arena/agents/brain.py
import numpy as np

class TinyNet:
    def __init__(self, w_in=None, w_out=None, input_size=14, hidden_size=16, output_size=4):
        """
        A simple two-layer neural network.
        - input_size: Number of input neurons (14 for this project)
        - hidden_size: Number of neurons in the hidden layer (16 for this project)
        - output_size: Number of output neurons (4 for this project)
        """
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights
        # Use a global RNG for reproducibility if desired, or np.random for simplicity now
        # For actual evolution, you'll want to manage the seed carefully.
        # self.rng = np.random.default_rng(seed) 
        
        if w_in is not None:
            self.w_in = np.array(w_in)
        else:
            self.w_in = np.random.uniform(-1, 1, (self.hidden_size, self.input_size))
            
        if w_out is not None:
            self.w_out = np.array(w_out)
        else:
            self.w_out = np.random.uniform(-1, 1, (self.output_size, self.hidden_size))

        self.fitness = 0.0 # To store fitness during evolution

    def __call__(self, x):
        """
        Forward pass through the network.
        x: Input vector (numpy array of shape (input_size,))
        Returns: Output vector (numpy array of shape (output_size,))
        """
        if not isinstance(x, np.ndarray):
            x = np.array(x, dtype=float)
        
        if x.shape[0] != self.input_size:
            raise ValueError(f"Input vector size {x.shape[0]} does not match expected size {self.input_size}")

        # Hidden layer: h = tanh(W_in @ x)
        h = np.tanh(self.w_in @ x)
        
        # Output layer: y = tanh(W_out @ h)
        y = np.tanh(self.w_out @ h) # Outputs are in range (-1, 1)
        return y

    def mutate(self, sigma=0.2, mutation_rate_weights=1.0, rng=None):
        """
        Creates a new mutated TinyNet.
        sigma: Standard deviation for Gaussian noise.
        mutation_rate_weights: Probability that any given weight matrix (w_in, w_out) gets mutated.
        rng: Optional numpy.random.Generator instance for reproducible randomness.
        """
        if rng is None:
            rng = np.random.default_rng()

        w_in_mutated = self.w_in.copy()
        w_out_mutated = self.w_out.copy()

        # Mutate input weights
        if rng.random() < mutation_rate_weights: # Mutate the whole matrix with some probability
            noise_in = rng.normal(0, sigma, self.w_in.shape)
            w_in_mutated += noise_in
        
        # Mutate output weights
        if rng.random() < mutation_rate_weights: # Mutate the whole matrix
            noise_out = rng.normal(0, sigma, self.w_out.shape)
            w_out_mutated += noise_out
        
        # Optionally clip weights if they grow too large, though tanh helps manage activation scale
        # w_in_mutated = np.clip(w_in_mutated, -max_weight, max_weight)
        # w_out_mutated = np.clip(w_out_mutated, -max_weight, max_weight)

        return TinyNet(w_in_mutated, w_out_mutated, self.input_size, self.hidden_size, self.output_size)

    @classmethod
    def crossover(cls, parent1, parent2, rng=None):
        """
        Performs uniform crossover between two parent TinyNets.
        rng: Optional numpy.random.Generator instance for reproducible randomness.
        """
        if rng is None:
            rng = np.random.default_rng()

        # Ensure parents have compatible shapes (could add assertions here)
        # For simplicity, assuming they are compatible.

        # Crossover for w_in
        mask_in = rng.random(parent1.w_in.shape) < 0.5
        w_in_child = np.where(mask_in, parent1.w_in, parent2.w_in)
        
        # Crossover for w_out
        mask_out = rng.random(parent1.w_out.shape) < 0.5
        w_out_child = np.where(mask_out, parent1.w_out, parent2.w_out)
        
        return cls(w_in_child, w_out_child, parent1.input_size, parent1.hidden_size, parent1.output_size)

    def get_genome_params(self):
        """Returns the weights as a tuple, suitable for saving."""
        return self.w_in, self.w_out

# Example usage (optional, for testing this file directly)
if __name__ == '__main__':
    # Test TinyNet
    input_vector = np.random.rand(14)
    net = TinyNet()
    output_vector = net(input_vector)
    print("Input:", input_vector)
    print("Output:", output_vector)
    print("w_in shape:", net.w_in.shape)   # Expected: (16, 14)
    print("w_out shape:", net.w_out.shape) # Expected: (4, 16)

    # Test mutation
    mutated_net = net.mutate(sigma=0.1)
    print("Original w_in[0,0]:", net.w_in[0,0])
    print("Mutated w_in[0,0]:", mutated_net.w_in[0,0])
    
    # Test crossover
    net2 = TinyNet()
    child_net = TinyNet.crossover(net, net2)
    print("Parent1 w_in[0,0]:", net.w_in[0,0])
    print("Parent2 w_in[0,0]:", net2.w_in[0,0])
    print("Child w_in[0,0]:", child_net.w_in[0,0]) # Should be one of the parent values

===== arena/arena.py =====
# evo_arena/arena/arena.py
import pygame
import math # Needed for atan2, degrees, radians, sqrt, cos, sin

class Arena:
    def __init__(self, width, height, wall_bounce_loss_factor=0.9):
        self.width = width
        self.height = height
        self.agents = []
        self.wall_bounce_loss_factor = wall_bounce_loss_factor
        self.game_time = 0.0 # For match timeout later

    def add_agent(self, agent):
        self.agents.append(agent)

    def get_alive_agents(self):
        return [agent for agent in self.agents if agent.is_alive()]

    def update(self, dt):
        self.game_time += dt
        
        # Get all agents that are currently alive
        # Process agent updates (movement, brain logic) first
        # Pass all agents for sensory input, but only alive ones are relevant for interaction
        current_all_agents_state = self.agents # Pass all, AgentBody.get_inputs will filter by .is_alive()
        
        for agent in self.agents:
            if agent.is_alive():
                 # Agent update (includes brain processing, action decisions, internal cooldowns)
                agent.update(dt, self.width, self.height, current_all_agents_state)
            else:
                agent.vx = 0 # Stop dead agents
                agent.vy = 0

        # Process firing and hit detection
        for idx, firing_agent in enumerate(self.agents):
            if not firing_agent.is_alive() or not firing_agent.is_firing_command:
                continue

            # If agent is commanding to fire AND its cooldown allowed it (is_firing_command is true)
            # The cooldown reset happens in AgentBody.update based on NN output or manual input.
            # So, if is_firing_command is true, it means an attempt to fire is made this tick.
            
            # For visualization, we might want to store that a shot was fired
            # firing_agent.shot_this_tick = True # Add this attribute to AgentBody if needed for drawing

            # Check for hits on other agents
            for target_idx, target_agent in enumerate(self.agents):
                if idx == target_idx or not target_agent.is_alive(): # Cannot shoot self or dead agents
                    continue

                # Check for friendly fire: by default, only hit enemies
                if firing_agent.team_id == target_agent.team_id: # Simple friendly fire check
                    # For some game modes, friendly fire might be enabled.
                    # For now, assume no friendly fire unless specified.
                    continue 

                # 1. Distance Check
                dx = target_agent.x - firing_agent.x
                dy = target_agent.y - firing_agent.y
                distance_sq = dx*dx + dy*dy # Use squared distance to avoid sqrt until necessary
                
                # Consider agent radii for effective range
                # effective_range = firing_agent.weapon_range + target_agent.radius 
                effective_range = firing_agent.weapon_range # Spec says "max range = 80", assume from center to center for simplicity
                                                          # Or edge of firing agent to center/edge of target.
                                                          # Let's use center-to-center for now.

                if distance_sq <= effective_range * effective_range:
                    distance = math.sqrt(distance_sq) # Now do sqrt
                    if distance == 0: continue # Avoid division by zero if agents are exactly on top of each other

                    # 2. Angle Check (120° cone = +/- 60° from agent's direction)
                    angle_to_target_rad = math.atan2(dy, dx)
                    agent_facing_rad = math.radians(firing_agent.angle_deg)
                    
                    # Relative angle of target with respect to agent's facing direction
                    relative_angle_rad = angle_to_target_rad - agent_facing_rad
                    
                    # Normalize to [-pi, pi]
                    relative_angle_rad = (relative_angle_rad + math.pi) % (2 * math.pi) - math.pi
                    
                    weapon_half_arc_rad = math.radians(firing_agent.weapon_arc_deg / 2.0)
                    
                    if abs(relative_angle_rad) <= weapon_half_arc_rad:
                        # HIT!
                        print(f"HIT! Agent {firing_agent.agent_id} (Team {firing_agent.team_id}) shot Agent {target_agent.agent_id} (Team {target_agent.team_id})")
                        target_agent.take_damage(firing_agent.weapon_damage)
                        # Potentially break here if weapon only hits one target per shot
                        # The spec doesn't specify (e.g. piercing), assume one hit per target in cone.
                        # If a single shot can hit multiple, don't break. Current loop handles this.

            # Reset the command flag for the firing agent after processing its shot for this tick
            # This is important if the brain/manual input keeps the fire signal high.
            # The agent's internal cooldown (weapon_cooldown_timer) is the primary gate.
            # is_firing_command is set by agent if cooldown permits.
            # If we reset it here, it means each fire *output* from NN leads to one check.
            # This seems reasonable.
            # firing_agent.is_firing_command = False # This might be too aggressive if agent already handles cooldown.
            # The agent already handles its cooldown and only sets is_firing_command if it CAN fire.
            # So, if is_firing_command is true, it means it has passed its cooldown check for this tick.
            # No need to reset it here, it will be re-evaluated by agent logic next tick.

        # Physics and Wall Collisions
        for agent in self.agents:
            if not agent.is_alive():
                continue # Skip physics for dead agents

            # Agent has already updated its vx, vy and potentially x, y in its own agent.update()
            # Now apply wall collisions based on the new prospective position (which is already its current x,y)

            # Store pre-collision speed for bounce reduction calculation
            # speed_before_collision = math.sqrt(agent.vx**2 + agent.vy**2) # Not directly used by spec for speed loss.

            collided_x = False
            collided_y = False

            # Left wall
            if agent.x - agent.radius < 0:
                agent.x = agent.radius
                agent.vx *= -self.wall_bounce_loss_factor
                collided_x = True
            # Right wall
            elif agent.x + agent.radius > self.width:
                agent.x = self.width - agent.radius
                agent.vx *= -self.wall_bounce_loss_factor
                collided_x = True
            
            # Top wall
            if agent.y - agent.radius < 0:
                agent.y = agent.radius
                agent.vy *= -self.wall_bounce_loss_factor
                collided_y = True
            # Bottom wall
            elif agent.y + agent.radius > self.height:
                agent.y = self.height - agent.radius
                agent.vy *= -self.wall_bounce_loss_factor
                collided_y = True
            
            # If collision occurred, apply 10% speed loss to the current_speed of the agent
            # The spec says "10% speed loss". This implies the magnitude of velocity.
            # Agent's current_speed might be a good proxy if it reflects the intended speed.
            # Or, more directly, scale the magnitude of (vx, vy).
            if (collided_x or collided_y) and hasattr(agent, 'base_speed'): # Check if it's a controllable agent
                # Reduce overall speed. The agent's next update() will re-calculate vx,vy based on new (potentially reduced) base_speed or current_speed.
                # This is tricky because "teleport-step" velocity is set by actions each frame.
                # A simple way to interpret "10% speed loss" is on the resulting velocity components *after* bounce.
                # This is already done by agent.vx *= -self.wall_bounce_loss_factor.
                # If it means a more persistent loss of thrusting capability, that's different.
                # The current implementation of agent.vx *= -factor effectively reduces the speed *for that bounce*.
                # If agent applies full thrust next frame, it goes back to full speed.
                # For now, let's stick to the current bounce logic. It reduces speed for the bounce reflection.
                pass


    def draw_bounds(self, screen):
        pygame.draw.rect(screen, (50, 50, 50), (0, 0, self.width, self.height), 2)

    def check_match_end_conditions(self):
        """
        Checks if the match should end.
        Returns: (is_over, winner_team_id_or_None_for_draw, message)
        """
        alive_agents = self.get_alive_agents()

        if not alive_agents:
            return True, None, "All agents eliminated (Draw or mutual destruction)"

        # Check for timeout (60s)
        if self.game_time >= 60.0:
            # Determine winner by HP or other tie-breaking rules if desired.
            # For now, timeout is a draw as per spec unless one team is standing.
            # "last team standing or 60 s timeout (draw)."
            # If timeout, and multiple teams still alive, it's a draw.
            # If timeout, and only one team alive, that team wins.
            
            teams_alive = set()
            for agent in alive_agents:
                teams_alive.add(agent.team_id)
            
            if len(teams_alive) > 1:
                return True, None, f"Timeout after {self.game_time:.1f}s. Draw."
            elif len(teams_alive) == 1:
                winner_team_id = list(teams_alive)[0]
                return True, winner_team_id, f"Timeout. Team {winner_team_id} wins (last standing)."
            else: # Should not happen if alive_agents is not empty
                return True, None, "Timeout. Draw (no teams identified)."


        # Check for last team standing
        if alive_agents:
            first_agent_team_id = alive_agents[0].team_id
            all_same_team = True
            for agent in alive_agents:
                if agent.team_id != first_agent_team_id:
                    all_same_team = False
                    break
            if all_same_team:
                return True, first_agent_team_id, f"Team {first_agent_team_id} is the last one standing!"
        
        return False, None, "Match ongoing"

    def reset(self):
        """Resets the arena for a new match."""
        self.game_time = 0.0
        # Agents themselves need to be reset (HP, position, etc.)
        # This is usually handled by recreating agents or calling a reset method on them.
        # For now, assuming agents are re-added/re-initialized by the evolution loop.
        # If running standalone matches, we might need an agent.reset()
        for agent in self.agents:
            # This is a placeholder; proper reset might involve re-initializing them to start positions/HP
            # For evolution, new agents (or reset ones) will be placed.
            # For now, if we call arena.reset(), it mainly resets game_time.
            # Agent state should be handled by whatever system starts the match.
            if hasattr(agent, 'initial_x') and hasattr(agent, 'initial_y') and hasattr(agent, 'initial_angle_deg'):
                agent.x = agent.initial_x
                agent.y = agent.initial_y
                agent.angle_deg = agent.initial_angle_deg
                agent.hp = agent.max_hp
                agent.weapon_cooldown_timer = 0.0
                agent.vx = 0.0
                agent.vy = 0.0
                agent.is_firing_command = False
            else: # Fallback if initial states not stored
                agent.hp = agent.max_hp 
                agent.weapon_cooldown_timer = 0.0

===== bak/my_agent_project_20250513_123955.md =====
===== Project File Tree =====
├── .gitignore
├── PLAN.md
├── README.md
├── agents
│   ├── body.py
│   └── brain.py
├── arena
│   └── arena.py
├── bak
├── config.py
├── evolve
│   └── evo.py
├── main.py
├── storage
│   └── persist.py
└── ui
    └── viewer.py

===== Code and Configuration Files =====

===== config.py =====


===== main.py =====
# evo_arena/main.py
import pygame # For key constants
# If you use TinyNet directly in main for testing, you might need this:
# from agents.brain import TinyNet 
from arena.arena import Arena
from agents.body import AgentBody
from ui.viewer import Viewer
# from storage import persist # For loading/saving brains later

# --- Configuration ---
ARENA_WIDTH = 800
ARENA_HEIGHT = 800
FPS = 50 # Game clock frequency, as per spec

AGENT_BASE_SPEED = 200  # "meters" (pixels) per second
AGENT_ROTATION_SPEED_DPS = 180  # degrees per second # <<< Ensure this constant name is clear
AGENT_RADIUS = 15
WALL_BOUNCE_LOSS_FACTOR = 0.9 # 10% speed loss

MANUAL_AGENT_COLOR = (0, 150, 255) # Blueish
DUMMY_AGENT_COLOR = (255, 100, 0) # Orangish

# Weapon defaults (can be overridden per agent if needed)
WEAPON_RANGE = 80
WEAPON_ARC_DEG = 120
WEAPON_COOLDOWN_TIME = 0.6 # seconds
WEAPON_DAMAGE = 10

def main():
    # Initialize Arena
    game_arena = Arena(ARENA_WIDTH, ARENA_HEIGHT, WALL_BOUNCE_LOSS_FACTOR)

    # Initialize Agents
    manual_agent = AgentBody(
        x=100, y=ARENA_HEIGHT / 2,
        angle_deg=0,
        base_speed=AGENT_BASE_SPEED,
        rotation_speed_dps=AGENT_ROTATION_SPEED_DPS, # <<< CORRECTED HERE
        radius=AGENT_RADIUS,
        color=MANUAL_AGENT_COLOR,
        agent_id="player",
        team_id=1, # Example team ID
        hp=100,
        brain=None, # No brain for manual agent
        weapon_range=WEAPON_RANGE,
        weapon_arc_deg=WEAPON_ARC_DEG,
        weapon_cooldown_time=WEAPON_COOLDOWN_TIME,
        weapon_damage=WEAPON_DAMAGE
    )
    game_arena.add_agent(manual_agent)

    dummy_target = AgentBody(
        x=ARENA_WIDTH - 100, y=ARENA_HEIGHT / 2,
        angle_deg=180,
        base_speed=0, # Dummy doesn't move on its own
        rotation_speed_dps=0, # <<< CORRECTED HERE
        radius=AGENT_RADIUS + 5, # Slightly larger
        color=DUMMY_AGENT_COLOR,
        agent_id="dummy",
        team_id=2, # Example different team ID
        hp=200, # Dummy might have more HP
        is_dummy=True,
        brain=None,
        weapon_range=0, # Dummy has no weapon
        weapon_arc_deg=0,
        weapon_cooldown_time=999,
        weapon_damage=0
    )
    game_arena.add_agent(dummy_target)

    # Initialize Viewer
    game_viewer = Viewer(ARENA_WIDTH, ARENA_HEIGHT)

    # Start the manual control loop
    # The viewer loop will now need to pass more info to arena.update if brains are active
    # For now, manual agent doesn't use brain, so its update doesn't strictly need all_agents etc.
    game_viewer.run_manual_loop(game_arena, manual_agent, FPS)

if __name__ == '__main__':
    main()

===== PLAN.md =====
Okay, this is a fantastic and well-structured plan! It's ambitious but definitely achievable for a focused 1-2 day hobby project. Let's break this down into a concrete development plan, keeping it lean.

I'll follow your "Day-by-day roadmap" as the main structure and flesh out the steps within each.

---

## Development Plan

**Core Principle:** Implement the simplest thing that works for each step. Get a feature working end-to-end before over-engineering or polishing. We'll stick to NumPy and PyGame as requested.

**Global Setup (Before Day 0):**

1.  **Project Setup:**
    *   Create a main project directory (e.g., `evo_arena`).
    *   Initialize a Git repository: `git init`.
    *   Create the basic directory structure: `arena/`, `agents/`, `evolve/`, `ui/`, `storage/`.
    *   Create empty `__init__.py` files in each subdirectory to make them Python packages.
    *   Create `main.py` at the root.
    *   Create placeholder files: `arena/arena.py`, `agents/brain.py`, `agents/body.py`, `evolve/evo.py`, `ui/viewer.py`, `storage/persist.py`.
2.  **Environment:**
    *   Ensure Python ≥ 3.10 is installed.
    *   Install dependencies: `pip install numpy pygame`.
3.  **Constants:**
    *   Create a `config.py` (or similar, or just put them at the top of `arena.py` for now) to hold game constants (arena size, agent speed, weapon specs, etc.) for easy tweaking.

---

### Phase 1: Manual Control & Basic Arena (Day 0 - Evening Goal)

**Goal:** Get a PyGame window showing an arena, one manually controlled agent, and a dummy target.

1.  **`arena/arena.py` - Basic Arena Logic:**
    *   `Arena` class:
        *   `__init__(self, width=800, height=800)`: Store dimensions.
        *   `add_agent(self, agent)`: Method to add agents (initially just one manual, one dummy).
        *   `update(self, dt)`:
            *   Loop through agents and call their `update` method (to be created in `body.py`).
            *   Implement wall collision:
                *   Check if agent's new position is outside bounds.
                *   If so, "bounce back" (invert appropriate velocity component) and reduce speed by 10%.
                *   Clamp position to be within bounds after bounce to prevent sticking.
    *   *Initial `dt` will be based on PyGame's clock tick.*

2.  **`agents/body.py` - Manual Agent Body:**
    *   `AgentBody` class:
        *   `__init__(self, x, y, angle, speed, team_id=0)`: Store position, orientation (angle in radians/degrees), speed, HP (e.g., 100).
        *   `vx`, `vy`: Current velocity components.
        *   `manual_control(self, keys)`:
            *   Update `vx`, `vy` based on arrow keys (e.g., up/down for forward/backward thrust, left/right for rotation).
            *   Rotation changes `angle`.
            *   Forward/backward thrust sets `vx = cos(angle) * speed`, `vy = sin(angle) * speed` (or modifies existing velocity if you want acceleration, but spec says "teleport-step"). For now, let's make actions set desired velocity for next step.
        *   `update(self, dt)`:
            *   `self.x += self.vx * dt`
            *   `self.y += self.vy * dt`
            *   *Initially, weapon logic is not needed.*

3.  **`ui/viewer.py` - PyGame Visualization:**
    *   `Viewer` class:
        *   `__init__(self, arena)`: Store arena reference. Initialize PyGame window.
        *   `draw_arena(self)`: Draw the arena boundary.
        *   `draw_agent(self, agent)`: Draw an agent as a triangle or circle with a line indicating direction.
        *   `run_manual_loop(self, manual_agent, dummy_agent)`:
            *   Main PyGame loop:
                *   Handle events (quit, keyboard input).
                *   Pass keyboard state to `manual_agent.manual_control()`.
                *   Call `arena.update(dt)` (where `dt` is from PyGame clock, e.g., `1/50.0`).
                *   Clear screen.
                *   Call `draw_arena()`.
                *   Draw `manual_agent` and `dummy_agent`.
                *   `pygame.display.flip()`.
                *   `pygame.time.Clock().tick(50)` (for 50 Hz).

4.  **`main.py` - Entry Point for Manual Mode:**
    *   Import necessary classes.
    *   Create `Arena` instance.
    *   Create one `AgentBody` for manual control, another `AgentBody` as a static dummy target.
    *   Add agents to the arena.
    *   Create `Viewer` instance.
    *   Call `viewer.run_manual_loop()`.

---

### Phase 2: Neural Net Agents & Headless Evolution (Day 1)

**Goal:** Implement `TinyNet`, integrate it with `AgentBody`, create the evolution loop, and run it headlessly for 20 generations. View fitness.

1.  **`agents/brain.py` - Neural Network:**
    *   Copy-paste the `TinyNet` class from the spec.
    *   Initialize `rng = np.random.default_rng(seed=None)` at module level or pass it around for determinism. (For now, `np.random` is fine for speed).

2.  **`agents/body.py` - AI Agent Body Enhancements:**
    *   Modify `AgentBody`:
        *   `__init__(...)`: Add `brain` parameter (a `TinyNet` instance). Add `weapon_cooldown_timer = 0`, `max_cooldown = 0.6`.
        *   `get_inputs(self, arena)`:
            *   This is crucial. It needs to find the nearest enemy and ally (if any).
            *   For 1v1, "ally" isn't relevant yet, but design for it.
            *   Calculate distances and bearings (sin/cos θ). Normalize to \[-1, 1].
            *   Implement "±1 if none in sight" logic.
            *   Own health / 100.
            *   Weapon ready? (1 if `weapon_cooldown_timer <= 0`, -1 otherwise).
            *   x-velocity, y-velocity (normalized/clamped).
            *   Bias neuron = 1.
            *   Return the 14-element NumPy array.
        *   `act(self, outputs)`:
            *   Take the 4 outputs from `TinyNet`.
            *   Threshold them (≥ 0 vs < 0).
            *   Translate to actions:
                *   Thrust: Adjust `vx`, `vy` based on current `angle` and `speed`.
                *   Strafe: Adjust `vx`, `vy` based on `angle + pi/2` or `angle - pi/2`.
                *   Rotate: Adjust `angle`.
                *   Fire: If `weapon_cooldown_timer <= 0`, set a flag `is_firing = True` and reset `weapon_cooldown_timer = self.max_cooldown`.
        *   Modify `update(self, dt)`:
            *   If `brain` exists:
                *   `inputs = self.get_inputs(arena)`
                *   `outputs = self.brain(inputs)`
                *   `self.act(outputs)`
            *   Decrement `weapon_cooldown_timer` if > 0.
            *   `self.x += self.vx * dt`
            *   `self.y += self.vy * dt`
            *   Reset `is_firing = False` after physics step.

3.  **`arena/arena.py` - Game Logic Enhancements:**
    *   Modify `Arena`:
        *   `update(self, dt)`:
            *   After agents update their positions:
                *   Handle weapon firing:
                    *   For each agent that `is_firing`:
                        *   Check for hits on other agents (enemies).
                        *   A hit occurs if an enemy is within the 120° cone and 80m range.
                        *   (Simple collision detection: distance check + angle check).
                        *   If hit, reduce target's HP by 10.
            *   Check match end conditions:
                *   One agent (or team) left.
                *   60s timeout (`self.game_time += dt`).
                *   Return a status (e.g., winner_id, 'draw').
        *   `reset(self)`: Method to clear agents, reset game time, etc., for new matches.
        *   Need a method to run a single match: `run_match(agent1_brain, agent2_brain, max_duration=60)`:
            *   Reset arena.
            *   Create two `AgentBody` instances, assign them the provided brains.
            *   Add them to the arena.
            *   Loop `max_duration / dt` times (e.g., 60s * 50Hz = 3000 steps):
                *   `arena.update(dt)`.
                *   If match ends, break and return winner/scores.
            *   Return final state (HP of both agents, who won).

4.  **`evolve/evo.py` - Evolution Loop:**
    *   `EvolutionOrchestrator` class:
        *   `__init__(self, population_size=64, num_elites=8, mutation_rate=0.9, mutation_sigma=0.2, target_fitness_stdev=0.03)`: Store params.
        *   `population`: A list of `TinyNet` instances (genomes).
        *   `initialize_population(self)`: Create `population_size` random `TinyNet`s.
        *   `run_tournament(self, genome1, genome2, arena_instance)`:
            *   Use `arena_instance.run_match(genome1.brain, genome2.brain)` (or directly pass genome if `TinyNet` is the genome).
            *   Return fitness components for genome1.
        *   `evaluate_population(self, arena_instance)`:
            *   For each genome in `population`:
                *   Initialize its fitness to 0.
                *   Play 4 matches (1v1) against random opponents from the current population.
                *   `fitness += num_wins + 0.1 * (HP_left - HP_enemy)`.
                *   Store fitness with the genome (e.g., `genome.fitness = ...`).
        *   `select_and_reproduce(self)`:
            *   Sort population by fitness.
            *   Copy top `num_elites` to the new population.
            *   For the remaining slots:
                *   With `mutation_rate` chance:
                    *   Select one parent (e.g., from top 50%).
                    *   `new_genome = parent.mutate(sigma=self.mutation_sigma)` (add `mutate` method to `TinyNet`).
                *   Else (crossover):
                    *   Select two parents.
                    *   `new_genome = TinyNet.crossover(parent1, parent2)` (add `crossover` method to `TinyNet`).
            *   Replace old population with new.
        *   `diversity_guard(self)`:
            *   Calculate stdev of fitness scores.
            *   If stdev < `target_fitness_stdev`, increase `self.mutation_sigma` (e.g., `*= 1.1`).
        *   `evolve(self, generations=20, arena_instance)`:
            *   Main loop:
                *   `self.evaluate_population(arena_instance)`.
                *   Log best/avg fitness (print to console, or basic CSV).
                *   `self.select_and_reproduce()`.
                *   `self.diversity_guard()`.
                *   Every 10 generations: call `persist.save_genome()` for the best genome.

5.  **`agents/brain.py` - `TinyNet` modifications:**
    *   `mutate(self, sigma)`:
        *   Create new `w_in_mutated = self.w_in + np.random.normal(0, sigma, self.w_in.shape)`.
        *   Create new `w_out_mutated = self.w_out + np.random.normal(0, sigma, self.w_out.shape)`.
        *   Return `TinyNet(w_in_mutated, w_out_mutated)`.
    *   `crossover(cls, parent1, parent2)` (class method):
        *   `w_in_child = np.where(np.random.rand(*parent1.w_in.shape) < 0.5, parent1.w_in, parent2.w_in)` (uniform crossover).
        *   `w_out_child = np.where(np.random.rand(*parent1.w_out.shape) < 0.5, parent1.w_out, parent2.w_out)`.
        *   Return `TinyNet(w_in_child, w_out_child)`.

6.  **`storage/persist.py` - Genome Storage:**
    *   `save_genome(genome, filename)`:
        *   `np.savez(filename, w_in=genome.w_in, w_out=genome.w_out, fitness=getattr(genome, 'fitness', -1))`.
    *   `load_genome(filename)`:
        *   `data = np.load(filename)`.
        *   Return `TinyNet(data['w_in'], data['w_out'])`. (Fitness can be retrieved if needed).

7.  **`main.py` - `train` Command:**
    *   Add argument parsing (e.g., `argparse`) for a `train` command.
    *   `if args.command == 'train'`:
        *   Create `Arena` instance.
        *   Create `EvolutionOrchestrator` instance.
        *   `orchestrator.initialize_population()`.
        *   `orchestrator.evolve(generations=args.iters, arena_instance=arena)`.
    *   To view fitness: For now, just print to console. A separate simple script can plot the CSV later if you log to CSV.

8.  **(Stretch) `storage/persist.py` - JSONL Replay:**
    *   In `Arena.run_match()`:
        *   At each step (or every N steps), collect state: `{'t': self.game_time, 'agents': [{'id': a.id, 'x': a.x, 'y': a.y, 'angle': a.angle, 'hp': a.hp} for a in self.agents]}`.
        *   Append this as a JSON line to a replay file.
    *   **(Stretch) `ui/viewer.py` or `main.py show_replay` - Simple Playback:**
        *   Read the JSONL file.
        *   For each entry, clear PyGame screen and draw agents based on the logged state.
        *   Control playback speed.

---

### Phase 3: Interactive Matches & Advanced Modes (Day 2)

**Goal:** Visualize 1v1 matches between specific agents, add FFA and Team modes.

1.  **`main.py` - `match` Command:**
    *   Add `match <genome1.npz> <genome2.npz>` command using `argparse`.
    *   Load the two genomes using `persist.load_genome()`.
    *   Create `Arena` instance.
    *   Create `Viewer` instance.
    *   `run_visual_match(self, arena, viewer, brain1, brain2)`:
        *   Similar to `Arena.run_match()` but integrates with `Viewer` for live display.
        *   In the loop:
            *   PyGame event handling.
            *   Agents get inputs, brains decide actions, bodies `act`.
            *   `arena.update(dt)`.
            *   `viewer.draw_everything()`.
            *   `pygame.display.flip()`.
            *   `pygame.time.Clock().tick(50)`.
            *   Check for match end.
    *   Use this to pit "gen 0 vs gen 200" (by saving gen 0 best and gen 200 best).

2.  **Game Mode Flags (`--ffa`, `--teams N`) in `main.py`:**
    *   Add these flags to `argparse` for the `train` command (and potentially for a new `show` command that can run different modes).

3.  **`arena/arena.py` & `evolve/evo.py` - FFA Mode:**
    *   **Arena:**
        *   `run_match()` needs to handle N agents, not just 2.
        *   Match ends when 1 agent is left or timeout.
        *   No concept of "enemy" vs "ally" in `AgentBody.get_inputs()`'s target selection for FFA; all others are "enemies".
    *   **Evolve:**
        *   `evaluate_population()` for FFA:
            *   Instead of 1v1, could do small FFA matches (e.g., 4 agents per match).
            *   Fitness is individual (e.g., survival time, damage dealt, KOs). Simpler: fitness = 1 if win, 0 if loss, + scaled HP_left.

4.  **`arena/arena.py` & `evolve/evo.py` - Team Mode:**
    *   **AgentBody:**
        *   `__init__`: Add `team_id`.
        *   `get_inputs()`: Must correctly identify nearest enemy *and* nearest ally based on `team_id`.
    *   **Arena:**
        *   `run_match()` needs to know team assignments.
        *   Match ends when one team is eliminated or timeout.
    *   **Evolve:**
        *   `evaluate_population()` for Team mode:
            *   Setup team matches (e.g., two teams of 3).
            *   **Shared team reward:** All members of the winning team get the same fitness outcome (e.g., win/loss points, team total HP difference).
            *   Example: `run_team_match(team1_genomes, team2_genomes, arena_instance)`.

5.  **(Stretch) Polish CLI, ZIP Checkpoints, README:**
    *   **CLI:** Refine `argparse` in `main.py` for all options (iterations, headless, game modes, file paths).
    *   **ZIP Checkpoints:** Modify `persist.save_genome` to potentially save to a zip if multiple files per checkpoint (e.g., genome + metadata JSON). For now, `.npz` is fine.
    *   **README.md:** Basic instructions on how to run `train`, `match`.

---

### Final Checklist Considerations (Throughout Development):

*   **Determinism:**
    *   Use `rng = np.random.default_rng(seed)`: Instantiate one `rng` object and pass it around or make it globally accessible (e.g., in `config.py`). Use `rng.uniform()`, `rng.normal()`, `rng.choice()` etc. This is key for reproducible replays and evolution.
    *   Fixed time step (50 Hz / 20 ms) is already planned.
*   **Configuration:**
    *   CLI args are a good start. For a 1-2 day project, a YAML file might be overkill but consider it if you find yourself with too many CLI options. Key parameters like mutation rates, population size, arena size should be easily tweakable.
*   **Clarity/Docs:**
    *   Good variable names.
    *   Comments for complex logic, especially in `arena.py` (hit detection, game rules) and `evolve.py` (selection, mutation).
    *   The current spec is a great "external" doc; internal code should be readable.

---

This plan breaks down the project into manageable chunks. Focus on getting one part working before moving to the next. For example, for Day 1, get *one* agent controlled by a *random* `TinyNet` moving in the arena *before* implementing the full evolution loop. Then add a second agent for 1v1, then the evolution.

Good luck, this looks like a super fun project!

===== README.md =====
Failed to read README.md: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

===== agents/body.py =====
# evo_arena/agents/body.py
import pygame
import math
import numpy as np # For brain inputs

# Assuming TinyNet might be passed in, or loaded
# from agents.brain import TinyNet # Not strictly needed here if brain is passed as object

class AgentBody:
    def __init__(self, x, y, angle_deg, base_speed, rotation_speed_dps,
                 radius=15, color=(0, 0, 255), agent_id="agent", team_id=0,
                 hp=100, brain=None, is_dummy=False,
                 weapon_range=80, weapon_arc_deg=120, weapon_cooldown_time=0.6, weapon_damage=10):
        
        self.agent_id = str(agent_id)
        self.team_id = int(team_id)
        self.x = float(x)
        self.y = float(y)
        self.angle_deg = float(angle_deg)
        self.radius = int(radius)
        self.color = color
        self.is_dummy = is_dummy
        self.brain = brain # This will be a TinyNet instance or None

        self.base_speed = float(base_speed)
        self.rotation_speed_dps = float(rotation_speed_dps) # Degrees per second for turning
        
        self.vx = 0.0
        self.vy = 0.0
        self.current_speed = 0.0 # Current forward/backward speed component

        self.max_hp = float(hp)
        self.hp = float(hp)
        
        # Weapon attributes
        self.weapon_range = float(weapon_range)
        self.weapon_arc_deg = float(weapon_arc_deg) # Centered on agent's forward direction
        self.weapon_cooldown_time = float(weapon_cooldown_time) # Seconds
        self.weapon_damage = int(weapon_damage)
        self.weapon_cooldown_timer = 0.0 # Counts down to 0 when ready
        self.is_firing_command = False # Set by brain/manual control, processed in arena

        # For manual control state (if brain is None)
        self._manual_thrust_forward = False
        self._manual_thrust_backward = False
        self._manual_rotate_left = False
        self._manual_rotate_right = False
        self._manual_fire = False
        
        # Clamping values for NN inputs/outputs if needed
        self.max_abs_velocity_component = self.base_speed # For normalizing velocity inputs

    def is_alive(self):
        return self.hp > 0

    def take_damage(self, amount):
        self.hp -= amount
        if self.hp < 0:
            self.hp = 0
            print(f"Agent {self.agent_id} defeated.")

    def manual_control(self, keys):
        if self.is_dummy or self.brain is not None: # Manual control only if no brain
            return

        self._manual_thrust_forward = keys[pygame.K_UP]
        self._manual_thrust_backward = keys[pygame.K_DOWN]
        self._manual_rotate_left = keys[pygame.K_LEFT]
        self._manual_rotate_right = keys[pygame.K_RIGHT]
        self._manual_fire = keys[pygame.K_SPACE] # Example: Space to fire

    def get_inputs(self, arena_width, arena_height, all_agents):
        """
        Generates the 14 input values for the neural network, normalized to [-1, 1].
        """
        inputs = np.zeros(14) # 14 inputs as per spec

        # Helper for normalization
        def normalize(value, min_val, max_val):
            if max_val == min_val: return 0.0 # Avoid division by zero
            return np.clip(2 * (value - min_val) / (max_val - min_val) - 1, -1.0, 1.0)

        # Helper to find nearest agent (enemy or ally)
        def find_nearest_agent_in_list(target_list):
            nearest_dist = float('inf')
            nearest_agent_obj = None
            for other_agent in target_list:
                if other_agent is self or not other_agent.is_alive():
                    continue
                dx = other_agent.x - self.x
                dy = other_agent.y - self.y
                dist = math.sqrt(dx**2 + dy**2) - self.radius - other_agent.radius # Edge to edge
                if dist < nearest_dist:
                    nearest_dist = dist
                    nearest_agent_obj = other_agent
            return nearest_agent_obj, nearest_dist

        # 1. Forward distance to nearest enemy
        # 2. Bearing to that enemy (sin θ, cos θ)
        enemies = [a for a in all_agents if a.team_id != self.team_id and a.is_alive()]
        nearest_enemy, enemy_dist = find_nearest_agent_in_list(enemies)
        
        max_view_dist = math.sqrt(arena_width**2 + arena_height**2) # Diagonal of arena

        if nearest_enemy:
            inputs[0] = normalize(enemy_dist, 0, max_view_dist) # Normalize distance
            
            dx_enemy = nearest_enemy.x - self.x
            dy_enemy = nearest_enemy.y - self.y
            angle_to_enemy_rad = math.atan2(dy_enemy, dx_enemy)
            
            # Bearing relative to agent's current orientation
            # Angle of agent: self.angle_deg (convert to rad)
            # Relative angle = angle_to_enemy - agent_angle
            relative_angle_rad = angle_to_enemy_rad - math.radians(self.angle_deg)
            # Normalize angle to [-pi, pi]
            relative_angle_rad = (relative_angle_rad + math.pi) % (2 * math.pi) - math.pi
            
            inputs[1] = math.sin(relative_angle_rad) # Already in [-1, 1]
            inputs[2] = math.cos(relative_angle_rad) # Already in [-1, 1]
        else:
            inputs[0] = 1.0 # No enemy in sight (or -1.0, spec says +/-1)
            inputs[1] = 0.0 # No bearing if no enemy
            inputs[2] = 0.0 

        # 3. Forward distance to nearest ally
        # 4. Bearing to that ally (sin θ, cos θ)
        allies = [a for a in all_agents if a.team_id == self.team_id and a is not self and a.is_alive()]
        nearest_ally, ally_dist = find_nearest_agent_in_list(allies)

        if nearest_ally:
            inputs[3] = normalize(ally_dist, 0, max_view_dist)
            
            dx_ally = nearest_ally.x - self.x
            dy_ally = nearest_ally.y - self.y
            angle_to_ally_rad = math.atan2(dy_ally, dx_ally)
            relative_angle_ally_rad = angle_to_ally_rad - math.radians(self.angle_deg)
            relative_angle_ally_rad = (relative_angle_ally_rad + math.pi) % (2 * math.pi) - math.pi
            
            inputs[4] = math.sin(relative_angle_ally_rad)
            inputs[5] = math.cos(relative_angle_ally_rad)
        else:
            inputs[3] = 1.0 # No ally in sight
            inputs[4] = 0.0
            inputs[5] = 0.0

        # 5. Own health / 100 (normalized to [0,1], spec says [-1,1], so scale if max_hp is 100)
        inputs[6] = normalize(self.hp, 0, self.max_hp) # Will be [-1,1] if hp can be 0 to max_hp

        # 6. Weapon ready? (1 = yes, –1 = cooling)
        inputs[7] = 1.0 if self.weapon_cooldown_timer <= 0 else -1.0

        # 7. x-velocity, y-velocity (clamped and normalized)
        # Max possible speed is base_speed for now.
        # These are world-frame velocities.
        inputs[8] = normalize(self.vx, -self.max_abs_velocity_component, self.max_abs_velocity_component)
        inputs[9] = normalize(self.vy, -self.max_abs_velocity_component, self.max_abs_velocity_component)
        
        # What are inputs 10, 11, 12? Spec shows 14 total.
        # My list:
        # 0: enemy_dist
        # 1: enemy_bearing_sin
        # 2: enemy_bearing_cos
        # 3: ally_dist
        # 4: ally_bearing_sin
        # 5: ally_bearing_cos
        # 6: health
        # 7: weapon_ready
        # 8: vx
        # 9: vy
        # --> This is 10 inputs. The spec image shows 14 inputs total.
        # Let's check the image again: "Forward distance", "Bearing (sin, cos)", "Forward distance (ally)", "Bearing (ally) (sin,cos)" = 6
        # "Own health" = 1
        # "Weapon ready" = 1
        # "x-velocity, y-velocity" = 2
        # "Bias neuron = 1" = 1
        # Total = 6+1+1+2+1 = 11.
        # The TinyNet code has `(16, 14)` for `w_in`. So 14 inputs.
        # "Inputs (all normalised to [-1, 1]) – 14 numbers total:"
        # 1. Forward distance to nearest enemy (±1 if none in sight) -> 1 number
        # 2. Bearing to that enemy (sin θ, cos θ) -> 2 numbers
        # 3. Forward distance to nearest ally (or ±1) -> 1 number
        # 4. Bearing to that ally (sin θ, cos θ) -> 2 numbers
        # 5. Own health / 100 -> 1 number
        # 6. Weapon ready? (1 = yes, –1 = cooling) -> 1 number
        # 7. x-velocity, y-velocity (clamped) -> 2 numbers
        # 8. Bias neuron = 1 -> 1 number
        # TOTAL: 1+2+1+2+1+1+2+1 = 11 inputs.
        #
        # The TinyNet diagram shows `14 x 16`.
        # The provided `TinyNet` python code `np.random.uniform(-1, 1, (16, 14))` implies 14 inputs.
        # Let's assume the list is the source of truth for the *meaning* of inputs,
        # and if there are more inputs needed by TinyNet, they are just unused or need clarification.
        # For now, I will make the input vector 14 long and pad the rest with 0, except the bias.
        #
        # Re-evaluating the spec text description for inputs:
        # 1. Fwd dist enemy (1)
        # 2. Bearing enemy (sin, cos) (2)
        # 3. Fwd dist ally (1)
        # 4. Bearing ally (sin, cos) (2)
        # 5. Own health (1)
        # 6. Weapon ready (1)
        # 7. x-velocity, y-velocity (2)
        # 8. Bias neuron (1)
        # This sums to 11 inputs.
        # The TinyNet init `self.w_in = w_in if w_in is not None else np.random.uniform(-1, 1, (16, 14))`
        # clearly expects 14 inputs.
        # The example calculation `14 × 16 + 16 × 4 = 320 parameters` also uses 14.
        #
        # Possible missing inputs / interpretation:
        # - Own angle (sin, cos)? (2)
        # - Distance to nearest wall in N directions?
        # - Time since last fired?
        #
        # For now, I'll stick to the 11 defined inputs and pad the input vector to 14.
        # The last one must be the bias neuron.
        
        # Inputs indices used so far: 0-9
        # We need 14 inputs. Last one is bias.
        # inputs[10], inputs[11], inputs[12] are currently unused. Let's set them to 0.
        inputs[10] = 0.0 # Unused / Placeholder
        inputs[11] = 0.0 # Unused / Placeholder
        inputs[12] = 0.0 # Unused / Placeholder

        # Last input: Bias neuron = 1
        inputs[13] = 1.0
        
        return inputs


    def perform_actions_from_outputs(self, outputs, dt):
        """
        Applies actions based on the 4 NN outputs.
        Outputs are real values, thresholded at 0.
        """
        if not self.is_alive():
            self.vx, self.vy = 0,0
            self.current_speed = 0
            return

        # Outputs: o1, o2, o3, o4
        # o1: thrust forward / back
        # o2: strafe left / right
        # o3: rotate left / right
        # o4: fire / hold fire

        # --- Rotation (o3) ---
        rotation_input = outputs[2]
        if rotation_input >= 0: # Rotate left
            self.angle_deg -= self.rotation_speed_dps * dt
        else: # Rotate right
            self.angle_deg += self.rotation_speed_dps * dt
        self.angle_deg %= 360

        # --- Movement (o1: thrust, o2: strafe) ---
        thrust_input = outputs[0]
        strafe_input = outputs[1]
        
        target_vx, target_vy = 0.0, 0.0
        agent_angle_rad = math.radians(self.angle_deg)

        # Thrust
        if thrust_input >= 0: # Thrust forward
            target_vx += self.base_speed * math.cos(agent_angle_rad)
            target_vy += self.base_speed * math.sin(agent_angle_rad)
        else: # Thrust backward
            target_vx -= (self.base_speed / 2) * math.cos(agent_angle_rad) # Slower backward
            target_vy -= (self.base_speed / 2) * math.sin(agent_angle_rad)

        # Strafe
        strafe_speed_factor = 0.75 # Strafe a bit slower than forward thrust
        if strafe_input >= 0: # Strafe left
            # Left is agent_angle_rad - PI/2
            strafe_angle_rad = agent_angle_rad - math.pi / 2
            target_vx += self.base_speed * strafe_speed_factor * math.cos(strafe_angle_rad)
            target_vy += self.base_speed * strafe_speed_factor * math.sin(strafe_angle_rad)
        else: # Strafe right
            # Right is agent_angle_rad + PI/2
            strafe_angle_rad = agent_angle_rad + math.pi / 2
            target_vx += self.base_speed * strafe_speed_factor * math.cos(strafe_angle_rad)
            target_vy += self.base_speed * strafe_speed_factor * math.sin(strafe_angle_rad)
        
        # For "teleport-step", we directly set velocity for this frame
        self.vx = target_vx
        self.vy = target_vy
        
        # Update current_speed for consistency if other parts rely on it (e.g. bounce reduction)
        # This is a bit tricky as vx, vy are now composed. We can store the magnitude.
        self.current_speed = math.sqrt(self.vx**2 + self.vy**2)
        if (thrust_input < 0 or (strafe_input != 0 and thrust_input == 0)): # If primarily moving backward or only strafing
             # Heuristic: if dominant movement is backward or purely sideways, consider speed negative for some logic
             # This is not well-defined by vx,vy. Let's use thrust_input to determine "forward" intent for current_speed sign.
             if thrust_input < 0 and self.current_speed > 0: # Moving backward
                 self.current_speed *= -0.5 # Match backward thrust factor
             elif thrust_input == 0 and strafe_input != 0:
                 pass # Pure strafe, speed is positive magnitude
        elif thrust_input < 0: # Moving backward
            self.current_speed *= -1 # This might be an oversimplification

        # --- Firing (o4) ---
        fire_input = outputs[3]
        if fire_input >= 0: # Fire
            if self.weapon_cooldown_timer <= 0:
                self.is_firing_command = True
                self.weapon_cooldown_timer = self.weapon_cooldown_time
            else:
                self.is_firing_command = False # Can't fire, on cooldown
        else: # Hold fire
            self.is_firing_command = False


    def update(self, dt, arena_width=None, arena_height=None, all_agents=None):
        if not self.is_alive():
            return

        # Decrement weapon cooldown
        if self.weapon_cooldown_timer > 0:
            self.weapon_cooldown_timer -= dt
            if self.weapon_cooldown_timer < 0:
                self.weapon_cooldown_timer = 0
        
        # --- Decision Making ---
        if self.brain:
            if arena_width is None or arena_height is None or all_agents is None:
                # This check is important because get_inputs needs these.
                # For simple manual mode, these might not be passed.
                # print("Warning: Agent brain active but not all arena info provided to update.")
                pass # Or raise error, or skip brain update for this frame
            else:
                inputs = self.get_inputs(arena_width, arena_height, all_agents)
                outputs = self.brain(inputs)
                self.perform_actions_from_outputs(outputs, dt)
        elif not self.is_dummy: # Manual control
            # Rotation
            if self._manual_rotate_left: self.angle_deg -= self.rotation_speed_dps * dt
            if self._manual_rotate_right: self.angle_deg += self.rotation_speed_dps * dt
            self.angle_deg %= 360
            
            # Velocity based on thrust
            current_manual_speed = 0.0
            if self._manual_thrust_forward: current_manual_speed = self.base_speed
            elif self._manual_thrust_backward: current_manual_speed = -self.base_speed / 2
            
            angle_rad = math.radians(self.angle_deg)
            self.vx = current_manual_speed * math.cos(angle_rad)
            self.vy = current_manual_speed * math.sin(angle_rad)
            self.current_speed = current_manual_speed # Store the signed speed

            # Firing for manual
            if self._manual_fire and self.weapon_cooldown_timer <= 0:
                self.is_firing_command = True
                self.weapon_cooldown_timer = self.weapon_cooldown_time
            else:
                self.is_firing_command = False # Reset if not firing or on cooldown
        else: # Dummy
            self.vx, self.vy = 0,0
            self.current_speed = 0
            self.is_firing_command = False

        # --- Position Update (Teleport-step) ---
        self.x += self.vx * dt
        self.y += self.vy * dt
        
        # Reset manual states (they are read fresh from keys each frame by viewer)
        # self._manual_thrust_forward = False ... (not needed here)

    def draw(self, screen):
        if not self.is_alive():
            # Optionally draw a wreck or nothing
            pygame.draw.circle(screen, (50,50,50), (int(self.x), int(self.y)), self.radius, 2) # Grey outline
            return

        # Draw body (circle)
        pygame.draw.circle(screen, self.color, (int(self.x), int(self.y)), self.radius)
        
        # Draw direction line
        angle_rad = math.radians(self.angle_deg)
        end_x = self.x + self.radius * math.cos(angle_rad)
        end_y = self.y + self.radius * math.sin(angle_rad)
        pygame.draw.line(screen, (255, 255, 255), (int(self.x), int(self.y)), (int(end_x), int(end_y)), 2)

        # Draw HP bar (optional)
        if self.max_hp > 0:
            hp_bar_width = self.radius * 2
            hp_bar_height = 5
            hp_bar_x = self.x - self.radius
            hp_bar_y = self.y - self.radius - hp_bar_height - 2 # Above agent

            current_hp_width = (self.hp / self.max_hp) * hp_bar_width
            pygame.draw.rect(screen, (255,0,0), (hp_bar_x, hp_bar_y, hp_bar_width, hp_bar_height)) # Red background
            pygame.draw.rect(screen, (0,255,0), (hp_bar_x, hp_bar_y, current_hp_width, hp_bar_height)) # Green foreground
        
        # Draw weapon cooldown indicator (optional)
        if self.weapon_cooldown_timer > 0:
            cooldown_arc_angle = (self.weapon_cooldown_timer / self.weapon_cooldown_time) * 360
            if cooldown_arc_angle > 0:
                try:
                    # Simplified: draw a small arc or circle segment
                    rect = pygame.Rect(self.x - self.radius/2, self.y - self.radius/2, self.radius, self.radius)
                    pygame.draw.arc(screen, (200,200,0), rect, 0, math.radians(cooldown_arc_angle), 2) # Yellow
                except TypeError as e: # pygame.draw.arc can be picky with small angles
                    # print(f"Warning: Pygame arc drawing issue: {e}")
                    pass


    def get_state_for_replay(self):
        """Returns a serializable dict of the agent's current state for replay."""
        return {
            'id': self.agent_id,
            'team_id': self.team_id,
            'x': round(self.x, 2),
            'y': round(self.y, 2),
            'angle_deg': round(self.angle_deg, 2),
            'hp': round(self.hp, 1),
            'is_firing': self.is_firing_command, # Log if trying to fire this frame
            'vx': round(self.vx, 2),
            'vy': round(self.vy, 2)
        }

===== agents/brain.py =====


===== arena/arena.py =====
# evo_arena/arena/arena.py
import pygame
import math # Needed for atan2, degrees, radians, sqrt, cos, sin

class Arena:
    def __init__(self, width, height, wall_bounce_loss_factor=0.9):
        self.width = width
        self.height = height
        self.agents = []
        self.wall_bounce_loss_factor = wall_bounce_loss_factor
        self.game_time = 0.0 # For match timeout later

    def add_agent(self, agent):
        self.agents.append(agent)

    def get_alive_agents(self):
        return [agent for agent in self.agents if agent.is_alive()]

    def update(self, dt):
        self.game_time += dt
        
        # Get all agents that are currently alive
        # Process agent updates (movement, brain logic) first
        # Pass all agents for sensory input, but only alive ones are relevant for interaction
        current_all_agents_state = self.agents # Pass all, AgentBody.get_inputs will filter by .is_alive()
        
        for agent in self.agents:
            if agent.is_alive():
                 # Agent update (includes brain processing, action decisions, internal cooldowns)
                agent.update(dt, self.width, self.height, current_all_agents_state)
            else:
                agent.vx = 0 # Stop dead agents
                agent.vy = 0

        # Process firing and hit detection
        for idx, firing_agent in enumerate(self.agents):
            if not firing_agent.is_alive() or not firing_agent.is_firing_command:
                continue

            # If agent is commanding to fire AND its cooldown allowed it (is_firing_command is true)
            # The cooldown reset happens in AgentBody.update based on NN output or manual input.
            # So, if is_firing_command is true, it means an attempt to fire is made this tick.
            
            # For visualization, we might want to store that a shot was fired
            # firing_agent.shot_this_tick = True # Add this attribute to AgentBody if needed for drawing

            # Check for hits on other agents
            for target_idx, target_agent in enumerate(self.agents):
                if idx == target_idx or not target_agent.is_alive(): # Cannot shoot self or dead agents
                    continue

                # Check for friendly fire: by default, only hit enemies
                if firing_agent.team_id == target_agent.team_id: # Simple friendly fire check
                    # For some game modes, friendly fire might be enabled.
                    # For now, assume no friendly fire unless specified.
                    continue 

                # 1. Distance Check
                dx = target_agent.x - firing_agent.x
                dy = target_agent.y - firing_agent.y
                distance_sq = dx*dx + dy*dy # Use squared distance to avoid sqrt until necessary
                
                # Consider agent radii for effective range
                # effective_range = firing_agent.weapon_range + target_agent.radius 
                effective_range = firing_agent.weapon_range # Spec says "max range = 80", assume from center to center for simplicity
                                                          # Or edge of firing agent to center/edge of target.
                                                          # Let's use center-to-center for now.

                if distance_sq <= effective_range * effective_range:
                    distance = math.sqrt(distance_sq) # Now do sqrt
                    if distance == 0: continue # Avoid division by zero if agents are exactly on top of each other

                    # 2. Angle Check (120° cone = +/- 60° from agent's direction)
                    angle_to_target_rad = math.atan2(dy, dx)
                    agent_facing_rad = math.radians(firing_agent.angle_deg)
                    
                    # Relative angle of target with respect to agent's facing direction
                    relative_angle_rad = angle_to_target_rad - agent_facing_rad
                    
                    # Normalize to [-pi, pi]
                    relative_angle_rad = (relative_angle_rad + math.pi) % (2 * math.pi) - math.pi
                    
                    weapon_half_arc_rad = math.radians(firing_agent.weapon_arc_deg / 2.0)
                    
                    if abs(relative_angle_rad) <= weapon_half_arc_rad:
                        # HIT!
                        print(f"HIT! Agent {firing_agent.agent_id} (Team {firing_agent.team_id}) shot Agent {target_agent.agent_id} (Team {target_agent.team_id})")
                        target_agent.take_damage(firing_agent.weapon_damage)
                        # Potentially break here if weapon only hits one target per shot
                        # The spec doesn't specify (e.g. piercing), assume one hit per target in cone.
                        # If a single shot can hit multiple, don't break. Current loop handles this.

            # Reset the command flag for the firing agent after processing its shot for this tick
            # This is important if the brain/manual input keeps the fire signal high.
            # The agent's internal cooldown (weapon_cooldown_timer) is the primary gate.
            # is_firing_command is set by agent if cooldown permits.
            # If we reset it here, it means each fire *output* from NN leads to one check.
            # This seems reasonable.
            # firing_agent.is_firing_command = False # This might be too aggressive if agent already handles cooldown.
            # The agent already handles its cooldown and only sets is_firing_command if it CAN fire.
            # So, if is_firing_command is true, it means it has passed its cooldown check for this tick.
            # No need to reset it here, it will be re-evaluated by agent logic next tick.

        # Physics and Wall Collisions
        for agent in self.agents:
            if not agent.is_alive():
                continue # Skip physics for dead agents

            # Agent has already updated its vx, vy and potentially x, y in its own agent.update()
            # Now apply wall collisions based on the new prospective position (which is already its current x,y)

            # Store pre-collision speed for bounce reduction calculation
            # speed_before_collision = math.sqrt(agent.vx**2 + agent.vy**2) # Not directly used by spec for speed loss.

            collided_x = False
            collided_y = False

            # Left wall
            if agent.x - agent.radius < 0:
                agent.x = agent.radius
                agent.vx *= -self.wall_bounce_loss_factor
                collided_x = True
            # Right wall
            elif agent.x + agent.radius > self.width:
                agent.x = self.width - agent.radius
                agent.vx *= -self.wall_bounce_loss_factor
                collided_x = True
            
            # Top wall
            if agent.y - agent.radius < 0:
                agent.y = agent.radius
                agent.vy *= -self.wall_bounce_loss_factor
                collided_y = True
            # Bottom wall
            elif agent.y + agent.radius > self.height:
                agent.y = self.height - agent.radius
                agent.vy *= -self.wall_bounce_loss_factor
                collided_y = True
            
            # If collision occurred, apply 10% speed loss to the current_speed of the agent
            # The spec says "10% speed loss". This implies the magnitude of velocity.
            # Agent's current_speed might be a good proxy if it reflects the intended speed.
            # Or, more directly, scale the magnitude of (vx, vy).
            if (collided_x or collided_y) and hasattr(agent, 'base_speed'): # Check if it's a controllable agent
                # Reduce overall speed. The agent's next update() will re-calculate vx,vy based on new (potentially reduced) base_speed or current_speed.
                # This is tricky because "teleport-step" velocity is set by actions each frame.
                # A simple way to interpret "10% speed loss" is on the resulting velocity components *after* bounce.
                # This is already done by agent.vx *= -self.wall_bounce_loss_factor.
                # If it means a more persistent loss of thrusting capability, that's different.
                # The current implementation of agent.vx *= -factor effectively reduces the speed *for that bounce*.
                # If agent applies full thrust next frame, it goes back to full speed.
                # For now, let's stick to the current bounce logic. It reduces speed for the bounce reflection.
                pass


    def draw_bounds(self, screen):
        pygame.draw.rect(screen, (50, 50, 50), (0, 0, self.width, self.height), 2)

    def check_match_end_conditions(self):
        """
        Checks if the match should end.
        Returns: (is_over, winner_team_id_or_None_for_draw, message)
        """
        alive_agents = self.get_alive_agents()

        if not alive_agents:
            return True, None, "All agents eliminated (Draw or mutual destruction)"

        # Check for timeout (60s)
        if self.game_time >= 60.0:
            # Determine winner by HP or other tie-breaking rules if desired.
            # For now, timeout is a draw as per spec unless one team is standing.
            # "last team standing or 60 s timeout (draw)."
            # If timeout, and multiple teams still alive, it's a draw.
            # If timeout, and only one team alive, that team wins.
            
            teams_alive = set()
            for agent in alive_agents:
                teams_alive.add(agent.team_id)
            
            if len(teams_alive) > 1:
                return True, None, f"Timeout after {self.game_time:.1f}s. Draw."
            elif len(teams_alive) == 1:
                winner_team_id = list(teams_alive)[0]
                return True, winner_team_id, f"Timeout. Team {winner_team_id} wins (last standing)."
            else: # Should not happen if alive_agents is not empty
                return True, None, "Timeout. Draw (no teams identified)."


        # Check for last team standing
        if alive_agents:
            first_agent_team_id = alive_agents[0].team_id
            all_same_team = True
            for agent in alive_agents:
                if agent.team_id != first_agent_team_id:
                    all_same_team = False
                    break
            if all_same_team:
                return True, first_agent_team_id, f"Team {first_agent_team_id} is the last one standing!"
        
        return False, None, "Match ongoing"

    def reset(self):
        """Resets the arena for a new match."""
        self.game_time = 0.0
        # Agents themselves need to be reset (HP, position, etc.)
        # This is usually handled by recreating agents or calling a reset method on them.
        # For now, assuming agents are re-added/re-initialized by the evolution loop.
        # If running standalone matches, we might need an agent.reset()
        for agent in self.agents:
            # This is a placeholder; proper reset might involve re-initializing them to start positions/HP
            # For evolution, new agents (or reset ones) will be placed.
            # For now, if we call arena.reset(), it mainly resets game_time.
            # Agent state should be handled by whatever system starts the match.
            if hasattr(agent, 'initial_x') and hasattr(agent, 'initial_y') and hasattr(agent, 'initial_angle_deg'):
                agent.x = agent.initial_x
                agent.y = agent.initial_y
                agent.angle_deg = agent.initial_angle_deg
                agent.hp = agent.max_hp
                agent.weapon_cooldown_timer = 0.0
                agent.vx = 0.0
                agent.vy = 0.0
                agent.is_firing_command = False
            else: # Fallback if initial states not stored
                agent.hp = agent.max_hp 
                agent.weapon_cooldown_timer = 0.0

===== evolve/evo.py =====


===== storage/persist.py =====


===== ui/viewer.py =====
# evo_arena/ui/viewer.py
import pygame
import math # For radians

class Viewer:
    def __init__(self, width, height, title="Evo Arena"):
        pygame.init()
        self.width = width
        self.height = height
        self.screen = pygame.display.set_mode((width, height))
        pygame.display.set_caption(title)
        self.clock = pygame.time.Clock()
        self.font = pygame.font.SysFont(None, 30) # Slightly larger font
        self.info_font = pygame.font.SysFont(None, 24)


    def draw_firing_cone(self, screen, agent):
        if not agent.is_alive() or not agent.is_firing_command : # or agent.weapon_cooldown_timer < agent.weapon_cooldown_time * 0.9: # Show cone briefly after firing
            # Only draw if actively trying to fire *this tick* or very recently fired.
            # The agent.is_firing_command is set if cooldown allows and input commands fire.
            # So this condition should be fine.
            return

        cone_color = (255, 255, 0, 100)  # Yellow, semi-transparent
        
        # Points for the cone polygon
        # Point 1: Agent's position
        p1 = (int(agent.x), int(agent.y))
        
        # Point 2: End of cone, left edge
        angle_left_rad = math.radians(agent.angle_deg - agent.weapon_arc_deg / 2.0)
        p2_x = agent.x + agent.weapon_range * math.cos(angle_left_rad)
        p2_y = agent.y + agent.weapon_range * math.sin(angle_left_rad)
        p2 = (int(p2_x), int(p2_y))

        # Point 3: End of cone, right edge
        angle_right_rad = math.radians(agent.angle_deg + agent.weapon_arc_deg / 2.0)
        p3_x = agent.x + agent.weapon_range * math.cos(angle_right_rad)
        p3_y = agent.y + agent.weapon_range * math.sin(angle_right_rad)
        p3 = (int(p3_x), int(p3_y))

        # Create a surface for transparency
        cone_surface = pygame.Surface((self.width, self.height), pygame.SRCALPHA)
        pygame.draw.polygon(cone_surface, cone_color, [p1, p2, p3])
        screen.blit(cone_surface, (0,0))


    def run_manual_loop(self, arena, manual_agent, fps): # `manual_agent` ref might not be needed if arena handles all
        running = True
        dt = 1.0 / fps
        match_message = ""
        game_over = False

        # Store initial agent states for reset if needed (simple way)
        for ag in arena.agents:
            ag.initial_x = ag.x
            ag.initial_y = ag.y
            ag.initial_angle_deg = ag.angle_deg
            ag.max_hp_initial = ag.max_hp # Store original max_hp

        while running:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_r and game_over: # Reset key
                        arena.reset() # Resets game time, agent HP/pos
                        for ag in arena.agents: # Full agent state reset
                            ag.x = ag.initial_x
                            ag.y = ag.initial_y
                            ag.angle_deg = ag.initial_angle_deg
                            ag.hp = ag.max_hp_initial # Use stored initial max_hp
                            ag.weapon_cooldown_timer = 0.0
                            ag.vx = 0.0
                            ag.vy = 0.0
                            ag.is_firing_command = False
                        game_over = False
                        match_message = ""


            if not game_over:
                keys = pygame.key.get_pressed()
                # Find the manual agent in the arena list if not passed directly
                # This assumes only one manual agent or clear identification
                for agent_in_arena in arena.agents:
                    if not agent_in_arena.is_dummy and agent_in_arena.brain is None: # Assuming this is our manual agent
                        agent_in_arena.manual_control(keys)
                        break # Process only one manual agent

                # Update game state
                arena.update(dt) 

                # Check for match end
                is_over, winner_team, message = arena.check_match_end_conditions()
                if is_over:
                    game_over = True
                    match_message = message
                    if winner_team is not None:
                        match_message += f" Winner: Team {winner_team}"
                    print(match_message)


            # Drawing
            self.screen.fill((30, 30, 30))  # Dark grey background
            arena.draw_bounds(self.screen)
            
            for agent_to_draw in arena.agents:
                agent_to_draw.draw(self.screen) # Agent draws itself (body, HP bar, etc.)
                if agent_to_draw.is_firing_command and agent_to_draw.is_alive(): # Draw cone if firing this tick
                    self.draw_firing_cone(self.screen, agent_to_draw)
            
            # Display Game Over Message
            if game_over:
                msg_surf = self.font.render(match_message, True, (255, 255, 0))
                msg_rect = msg_surf.get_rect(center=(self.width / 2, self.height / 2))
                self.screen.blit(msg_surf, msg_rect)
                
                reset_surf = self.info_font.render("Press 'R' to reset", True, (200, 200, 200))
                reset_rect = reset_surf.get_rect(center=(self.width / 2, self.height / 2 + 40))
                self.screen.blit(reset_surf, reset_rect)

            # Display FPS and Game Time (optional)
            # current_fps = self.clock.get_fps()
            # fps_text = self.info_font.render(f"FPS: {current_fps:.1f}", True, (220, 220, 220))
            # self.screen.blit(fps_text, (10, 10))
            time_text = self.info_font.render(f"Time: {arena.game_time:.1f}s", True, (220, 220, 220))
            self.screen.blit(time_text, (10, 10))


            pygame.display.flip()
            self.clock.tick(fps)

        pygame.quit()



===== evolve/evo.py =====


===== storage/persist.py =====
# evo_arena/storage/persist.py
import numpy as np
import os
import json # For match replays later

from agents.brain import TinyNet # To reconstruct TinyNet objects

def save_genome(genome_brain, filename_prefix="genome", directory="storage/genomes", generation=None, fitness=None):
    """
    Saves the genome (weights of a TinyNet) to a .npz file.
    """
    if not os.path.exists(directory):
        os.makedirs(directory)

    w_in, w_out = genome_brain.get_genome_params()
    
    # Construct filename
    if generation is not None:
        filename = f"{filename_prefix}_g{generation:05d}"
    else:
        filename = filename_prefix
    
    if fitness is not None:
        filename += f"_fit{fitness:.3f}"
    filename += ".npz"
    
    filepath = os.path.join(directory, filename)
    
    save_data = {'w_in': w_in, 'w_out': w_out}
    if hasattr(genome_brain, 'fitness'):
      save_data['fitness'] = genome_brain.fitness
    elif fitness is not None:
      save_data['fitness'] = fitness


    np.savez(filepath, **save_data)
    print(f"Saved genome to {filepath}")
    return filepath

def load_genome(filepath, input_size=14, hidden_size=16, output_size=4):
    """
    Loads a genome from a .npz file and reconstructs a TinyNet.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Genome file not found: {filepath}")
        
    data = np.load(filepath)
    w_in = data['w_in']
    w_out = data['w_out']
    
    loaded_brain = TinyNet(w_in, w_out, input_size, hidden_size, output_size)
    if 'fitness' in data:
        loaded_brain.fitness = float(data['fitness'])
        
    print(f"Loaded genome from {filepath}")
    return loaded_brain

# --- Match Replay Functions (for later, as per spec section 6) ---
REPLAY_DIR = "storage/replays"

def start_match_replay(filename_prefix="match_replay"):
    if not os.path.exists(REPLAY_DIR):
        os.makedirs(REPLAY_DIR)
    
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
    filename = f"{filename_prefix}_{timestamp}.jsonl"
    filepath = os.path.join(REPLAY_DIR, filename)
    return open(filepath, 'w'), filepath # Return file handle and path

def record_arena_snapshot(replay_file_handle, game_time, agents_data):
    """
    Records a snapshot of the arena state to the replay file.
    agents_data: A list of dicts, each representing an agent's state.
    """
    snapshot = {
        't': round(game_time, 3),
        'state': agents_data 
    }
    replay_file_handle.write(json.dumps(snapshot) + '\n')

def close_match_replay(replay_file_handle):
    if replay_file_handle:
        replay_file_handle.close()

# Example usage (optional, for testing this file directly)
if __name__ == '__main__':
    # Test saving and loading
    test_brain = TinyNet()
    test_brain.fitness = 123.456
    
    # Create dummy directories if they don't exist
    if not os.path.exists("storage/genomes"):
        os.makedirs("storage/genomes")

    filepath = save_genome(test_brain, filename_prefix="test_dummy", generation=0, fitness=test_brain.fitness)
    loaded_brain = load_genome(filepath)
    
    assert np.array_equal(test_brain.w_in, loaded_brain.w_in)
    assert np.array_equal(test_brain.w_out, loaded_brain.w_out)
    assert hasattr(loaded_brain, 'fitness') and loaded_brain.fitness == test_brain.fitness
    print("Save/Load test successful.")

    # Test replay (rudimentary)
    # file_handle, replay_path = start_match_replay()
    # print(f"Replay file started: {replay_path}")
    # record_arena_snapshot(file_handle, 0.02, [{'id': 'a1', 'x': 10, 'y': 20, 'hp': 100}])
    # record_arena_snapshot(file_handle, 0.04, [{'id': 'a1', 'x': 12, 'y': 22, 'hp': 90}])
    # close_match_replay(file_handle)
    # print("Replay test successful.")

===== ui/viewer.py =====
# evo_arena/ui/viewer.py
import pygame
import math # For radians

class Viewer:
    def __init__(self, width, height, title="Evo Arena"):
        pygame.init()
        self.width = width
        self.height = height
        self.screen = pygame.display.set_mode((width, height))
        pygame.display.set_caption(title)
        self.clock = pygame.time.Clock()
        self.font = pygame.font.SysFont(None, 30) # Slightly larger font
        self.info_font = pygame.font.SysFont(None, 24)


    def draw_firing_cone(self, screen, agent):
        if not agent.is_alive() or not agent.is_firing_command : # or agent.weapon_cooldown_timer < agent.weapon_cooldown_time * 0.9: # Show cone briefly after firing
            # Only draw if actively trying to fire *this tick* or very recently fired.
            # The agent.is_firing_command is set if cooldown allows and input commands fire.
            # So this condition should be fine.
            return

        cone_color = (255, 255, 0, 100)  # Yellow, semi-transparent
        
        # Points for the cone polygon
        # Point 1: Agent's position
        p1 = (int(agent.x), int(agent.y))
        
        # Point 2: End of cone, left edge
        angle_left_rad = math.radians(agent.angle_deg - agent.weapon_arc_deg / 2.0)
        p2_x = agent.x + agent.weapon_range * math.cos(angle_left_rad)
        p2_y = agent.y + agent.weapon_range * math.sin(angle_left_rad)
        p2 = (int(p2_x), int(p2_y))

        # Point 3: End of cone, right edge
        angle_right_rad = math.radians(agent.angle_deg + agent.weapon_arc_deg / 2.0)
        p3_x = agent.x + agent.weapon_range * math.cos(angle_right_rad)
        p3_y = agent.y + agent.weapon_range * math.sin(angle_right_rad)
        p3 = (int(p3_x), int(p3_y))

        # Create a surface for transparency
        cone_surface = pygame.Surface((self.width, self.height), pygame.SRCALPHA)
        pygame.draw.polygon(cone_surface, cone_color, [p1, p2, p3])
        screen.blit(cone_surface, (0,0))


    def run_manual_loop(self, arena, manual_agent, fps): # `manual_agent` ref might not be needed if arena handles all
        running = True
        dt = 1.0 / fps
        match_message = ""
        game_over = False

        # Store initial agent states for reset if needed (simple way)
        for ag in arena.agents:
            ag.initial_x = ag.x
            ag.initial_y = ag.y
            ag.initial_angle_deg = ag.angle_deg
            ag.max_hp_initial = ag.max_hp # Store original max_hp

        while running:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    running = False
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_r and game_over: # Reset key
                        arena.reset() # Resets game time, agent HP/pos
                        for ag in arena.agents: # Full agent state reset
                            ag.x = ag.initial_x
                            ag.y = ag.initial_y
                            ag.angle_deg = ag.initial_angle_deg
                            ag.hp = ag.max_hp_initial # Use stored initial max_hp
                            ag.weapon_cooldown_timer = 0.0
                            ag.vx = 0.0
                            ag.vy = 0.0
                            ag.is_firing_command = False
                        game_over = False
                        match_message = ""


            if not game_over:
                keys = pygame.key.get_pressed()
                # Find the manual agent in the arena list if not passed directly
                # This assumes only one manual agent or clear identification
                for agent_in_arena in arena.agents:
                    if not agent_in_arena.is_dummy and agent_in_arena.brain is None: # Assuming this is our manual agent
                        agent_in_arena.manual_control(keys)
                        break # Process only one manual agent

                # Update game state
                arena.update(dt) 

                # Check for match end
                is_over, winner_team, message = arena.check_match_end_conditions()
                if is_over:
                    game_over = True
                    match_message = message
                    if winner_team is not None:
                        match_message += f" Winner: Team {winner_team}"
                    print(match_message)


            # Drawing
            self.screen.fill((30, 30, 30))  # Dark grey background
            arena.draw_bounds(self.screen)
            
            for agent_to_draw in arena.agents:
                agent_to_draw.draw(self.screen) # Agent draws itself (body, HP bar, etc.)
                if agent_to_draw.is_firing_command and agent_to_draw.is_alive(): # Draw cone if firing this tick
                    self.draw_firing_cone(self.screen, agent_to_draw)
            
            # Display Game Over Message
            if game_over:
                msg_surf = self.font.render(match_message, True, (255, 255, 0))
                msg_rect = msg_surf.get_rect(center=(self.width / 2, self.height / 2))
                self.screen.blit(msg_surf, msg_rect)
                
                reset_surf = self.info_font.render("Press 'R' to reset", True, (200, 200, 200))
                reset_rect = reset_surf.get_rect(center=(self.width / 2, self.height / 2 + 40))
                self.screen.blit(reset_surf, reset_rect)

            # Display FPS and Game Time (optional)
            # current_fps = self.clock.get_fps()
            # fps_text = self.info_font.render(f"FPS: {current_fps:.1f}", True, (220, 220, 220))
            # self.screen.blit(fps_text, (10, 10))
            time_text = self.info_font.render(f"Time: {arena.game_time:.1f}s", True, (220, 220, 220))
            self.screen.blit(time_text, (10, 10))


            pygame.display.flip()
            self.clock.tick(fps)

        pygame.quit()

